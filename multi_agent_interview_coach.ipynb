{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Agent Interview Coach\n",
        "\n",
        "Мультиагентная система для проведения технического интервью: агенты Interviewer, Observer и Evaluator, скрытая рефлексия, диалог о проектах и технические вопросы, финальный структурированный фидбэк"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.2.7)\n",
            "Requirement already satisfied: langchain-openai in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.1.7)\n",
            "Requirement already satisfied: langchain-anthropic in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.3.1)\n",
            "Requirement already satisfied: langchain-mistralai in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.1.1)\n",
            "Requirement already satisfied: langchain-google-genai in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.2.0)\n",
            "Requirement already satisfied: langgraph in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.0.7)\n",
            "Requirement already satisfied: python-dotenv in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.1.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.7 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain) (1.2.7)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain) (2.12.4)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langgraph) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langgraph) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langgraph) (0.3.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (0.6.6)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.7->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph) (1.12.2)\n",
            "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.6)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (0.24.0)\n",
            "Requirement already satisfied: anyio in c:\\users\\asus\\anaconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (4.10.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\asus\\anaconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\asus\\anaconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\users\\asus\\anaconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-openai) (2.16.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in c:\\users\\asus\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.9.1)\n",
            "Requirement already satisfied: anthropic<1.0.0,>=0.75.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-anthropic) (0.77.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from anthropic<1.0.0,>=0.75.0->langchain-anthropic) (0.17.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.3.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-mistralai) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.15.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-mistralai) (0.22.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tokenizers<1.0.0,>=0.15.1->langchain-mistralai) (1.3.5)\n",
            "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1.0.0,>=0.15.1->langchain-mistralai) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1.0.0,>=0.15.1->langchain-mistralai) (2025.10.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1.0.0,>=0.15.1->langchain-mistralai) (1.2.0)\n",
            "Requirement already satisfied: shellingham in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1.0.0,>=0.15.1->langchain-mistralai) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1.0.0,>=0.15.1->langchain-mistralai) (0.21.1)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.56.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-google-genai) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.48.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (15.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: cryptography>=38.0.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (46.0.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (2.5.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from cryptography>=38.0.3->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.0.0)\n",
            "Requirement already satisfied: pycparser in c:\\users\\asus\\anaconda3\\lib\\site-packages (from cffi>=2.0.0->cryptography>=38.0.3->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.23)\n",
            "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm>4->openai<3.0.0,>=1.109.1->langchain-openai) (0.4.6)\n",
            "Requirement already satisfied: click>=8.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers<1.0.0,>=0.15.1->langchain-mistralai) (8.2.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install langchain langchain-openai langchain-anthropic langchain-mistralai langchain-google-genai langgraph python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from typing import TypedDict, Annotated, Sequence\n",
        "from datetime import datetime\n",
        "import re\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Выбор модели\n",
        "\n",
        "<small>загрузка .env, выбор провайдера LLM и инициализация модели.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM: deepseek\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "LLM_PROVIDER = os.getenv(\"LLM_PROVIDER\", \"deepseek\")\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
        "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\", \"\")\n",
        "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\", \"\")\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"\")\n",
        "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\", \"\")\n",
        "\n",
        "def get_llm():\n",
        "    if LLM_PROVIDER == \"anthropic\":\n",
        "        from langchain_anthropic import ChatAnthropic\n",
        "        return ChatAnthropic(model=\"claude-3-5-sonnet-20241022\", api_key=ANTHROPIC_API_KEY, temperature=0.5)\n",
        "    \n",
        "    elif LLM_PROVIDER == \"gigachat\":\n",
        "        from langchain_openai import ChatOpenAI\n",
        "        return ChatOpenAI(model=\"GigaChat\", base_url=os.getenv(\"GIGACHAT_BASE_URL\", \"https://gigachat.dev/api/v1\"), api_key=os.getenv(\"GIGACHAT_API_KEY\", OPENAI_API_KEY), temperature=0.5)\n",
        "    \n",
        "    elif LLM_PROVIDER == \"mistral\":\n",
        "        from langchain_mistralai import ChatMistralAI\n",
        "        model = os.getenv(\"MISTRAL_MODEL\", \"mistral-small-latest\")\n",
        "        return ChatMistralAI(model=model, api_key=MISTRAL_API_KEY, temperature=0.5)\n",
        "    \n",
        "    elif LLM_PROVIDER == \"gemini\":\n",
        "        from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "        model = os.getenv(\"GOOGLE_MODEL\", \"gemini-1.5-flash\")\n",
        "        return ChatGoogleGenerativeAI(model=model, api_key=GOOGLE_API_KEY, temperature=0.5)\n",
        "    \n",
        "    elif LLM_PROVIDER == \"deepseek\":\n",
        "        from langchain_openai import ChatOpenAI\n",
        "        model = os.getenv(\"DEEPSEEK_MODEL\", \"deepseek-chat\")\n",
        "        return ChatOpenAI(model=model, base_url=\"https://api.deepseek.com\", api_key='API_KEY', temperature=0.5)\n",
        "    \n",
        "    else:\n",
        "        from langchain_openai import ChatOpenAI\n",
        "        return ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.5)\n",
        "\n",
        "llm = get_llm()\n",
        "print(f\"LLM: {LLM_PROVIDER}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Устойчивость к вбросам \n",
        "\n",
        "<small>класс проверки off-topic по ключевым словам и редирект-сообщение для возврата к интервью.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RobustnessValidator:\n",
        "    OFF_TOPIC_KEYWORDS = [\n",
        "        \"погода\", \"weather\", \"политика\", \"politics\", \"спорт\", \"sport\",\n",
        "        \"футбол\", \"football\", \"кино\", \"movie\", \"музыка\", \"music\",\n",
        "        \"еда\", \"food\", \"путешествие\", \"travel\"\n",
        "    ]\n",
        "    ON_TOPIC_KEYWORDS = [\n",
        "        \"код\", \"code\", \"программ\", \"program\", \"разработка\", \"develop\",\n",
        "        \"язык\", \"language\", \"библиотека\", \"library\", \"фреймворк\", \"framework\",\n",
        "        \"база данных\", \"database\", \"алгоритм\", \"algorithm\", \"тест\", \"test\",\n",
        "        \"архитектура\", \"architecture\", \"проект\", \"project\", \"опыт\", \"experience\",\n",
        "        \"python\", \"java\", \"sql\", \"api\", \"git\"\n",
        "    ]\n",
        "    EVASION_PATTERNS = [\n",
        "        r\"давай(те)?\\s+поговорим\\s+о\", r\"а\\s+можно\\s+о\", r\"лучше\\s+расскажи\", r\"смени\\s+тему\"\n",
        "    ]\n",
        "\n",
        "    @staticmethod\n",
        "    def is_off_topic(message: str) -> bool:\n",
        "        if not message or len(message.strip().split()) < 3:\n",
        "            return False\n",
        "        low = message.lower()\n",
        "        off = sum(1 for k in RobustnessValidator.OFF_TOPIC_KEYWORDS if k in low)\n",
        "        on = sum(1 for k in RobustnessValidator.ON_TOPIC_KEYWORDS if k in low)\n",
        "        if off > 0 and on == 0:\n",
        "            return True\n",
        "        for p in RobustnessValidator.EVASION_PATTERNS:\n",
        "            if re.search(p, low):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def get_redirect_message() -> str:\n",
        "        return (\n",
        "            \"Спасибо за ваш ответ. Давайте вернёмся к техническим вопросам интервью — \"\n",
        "            \"так я смогу лучше оценить ваши навыки для позиции.\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Состояние графа \n",
        "\n",
        "<small>определение InterviewState (messages, turns, internal_thoughts, difficulty, topics_covered и др.) для графа LangGraph.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class InterviewState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], \"история сообщений\"]\n",
        "    participant_name: str\n",
        "    position: str\n",
        "    grade: str\n",
        "    experience: str\n",
        "    candidate_name: str\n",
        "    internal_thoughts: Annotated[list[str], \"логи внутренних мыслей агентов\"]\n",
        "    turns: Annotated[list[dict], \"зафиксированные ходы для лога\"]\n",
        "    current_difficulty: str  \n",
        "    topics_covered: Annotated[list[str], \"темы, уже затронутые\"]\n",
        "    stop_requested: bool\n",
        "    final_feedback: str\n",
        "    off_topic_count: int \n",
        "    performance_history: Annotated[list[float], \"оценки Evaluator по ходам 0..1\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Промпты агентов\n",
        "\n",
        "<small>системные промпты для Observer, Interviewer и Evaluator (формат THOUGHT/INSTRUCTION/DIFFICULTY/TOPICS и оценка 0–1).</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "OBSERVER_SYSTEM = \"\"\"Ты — Observer (Наблюдатель) в техническом интервью. Ты не общаешься с кандидатом напрямую.\n",
        "Твоя задача:\n",
        "1. Проанализировать последний ответ кандидата: точность фактов, уверенность, полнота.\n",
        "2. Определить, не ушёл ли разговор off-topic (погода, личное и т.д.) или кандидат не выдал ли ложные факты (галлюцинации). В таких случаях предложи вежливо вернуть беседу в русло интервью.\n",
        "3. Учитывая контекст (позиция: {position}, грейд: {grade}, опыт: {experience}) и уже затронутые темы {topics_covered}, решить:\n",
        "   - какую тему задать дальше (не повторять вопросы, на которые уже был ответ);\n",
        "   - повысить сложность (current_difficulty -> hard), оставить (medium) или понизить (easy), если кандидат \"плывёт\".\n",
        "4. Фаза интервью: пока число обменов репликами (turns) меньше 2 — веди разговор о проектах и опыте (INSTRUCTION: вопросы о проектах, технологиях в проектах, роли). После 2 обменов о проектах (turns >= 2) — переходи к техническим вопросам по позиции (Python, БД, API и т.д.). Всего около 8 сообщений от кандидата за интервью, за это время нужно проверить его.\n",
        "5. Если кандидат ушёл от ответа, задав встречный вопрос (например, «а как у вас?», «а что вы имеете в виду?»): в INSTRUCTION укажи, что интервьюер должен кратко ответить на вопрос кандидата (1–2 предложения), а затем вежливо вернуть разговор к исходному вопросу и повторить или переформулировать его.\n",
        "6. Выдать ответ СТРОГО в формате (все блоки обязательны):\n",
        "   THOUGHT: [твои выводы и рекомендации на 1-3 предложения]\n",
        "   INSTRUCTION: [что сказать/спросить дальше: конкретный вопрос или действие]\n",
        "   DIFFICULTY: [easy | medium | hard] — следующая сложность вопросов\n",
        "   TOPICS: [тема текущего/следующего вопроса, одна или через запятую]\n",
        "Отвечай только на русском. Не придумывай факты.\"\"\"\n",
        "\n",
        "INTERVIEWER_SYSTEM = \"\"\"Ты — Interviewer (Интервьюер) на техническом интервью. Ты ведёшь диалог с кандидатом.\n",
        "Контекст: позиция {position}, грейд {grade}, опыт кандидата: {experience}. Имя кандидата: {candidate_name}.\n",
        "Ты получил от Observer инструкцию (INSTRUCTION). Выполни её: задай один вопрос или сделай короткий комментарий, затем задай следующий вопрос.\n",
        "Правила:\n",
        "- Сначала 2 обмена о проектах и опыте (расскажите подробнее о проектах, какие технологии использовали), затем переходи к техническим вопросам по позиции.\n",
        "- Говори от первого лица (интервьюер). Один блок реплики — без разметки, без \"Interviewer:\".\n",
        "- Если инструкция говорит вернуть беседу в русло — вежливо переведи разговор обратно к теме интервью.\n",
        "- Не задавай вопросов, на которые кандидат уже ответил в этой беседе.\n",
        "- Адаптируй тон: при слабом ответе можно дать подсказку или упростить следующий вопрос.\n",
        "- Если кандидат вместо ответа задал встречный вопрос (уклонился от ответа): сначала кратко (1–2 предложения) ответь на его вопрос, затем вежливо верни разговор к своему исходному вопросу и повтори или переформулируй его, чтобы получить ответ.\n",
        "Отвечай только текстом реплики для кандидата, без префиксов.\"\"\"\n",
        "\n",
        "EVALUATOR_SYSTEM = \"\"\"Ты — Evaluator (Оценщик) в техническом интервью. Ты не общаешься с кандидатом.\n",
        "Оцени ответ кандидата на заданный вопрос: фактическая корректность (correct/partial/incorrect), полнота, глубина.\n",
        "Дай числовую оценку 0.0–1.0. Если ответ с ошибками — укажи правильный ответ в блоке ПРАВИЛЬНЫЙ_ОТВЕТ:.\n",
        "Формат ответа:\n",
        "ОЦЕНКА: число 0.0–1.0\n",
        "КОРРЕКТНОСТЬ: correct | partial | incorrect\n",
        "КОММЕНТАРИЙ: 1–2 предложения\n",
        "ПРАВИЛЬНЫЙ_ОТВЕТ: (если нужен — кратко)\n",
        "Отвечай на русском.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Скрытая рефлексия\n",
        "\n",
        "<small>узел Observer: анализ последнего ответа кандидата, вызов LLM, парсинг DIFFICULTY/TOPICS и обновление internal_thoughts.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _format_history(messages: Sequence[BaseMessage], max_last: int = 12) -> str:\n",
        "    parts = []\n",
        "    for m in list(messages)[-max_last:]:\n",
        "        if isinstance(m, HumanMessage):\n",
        "            parts.append(f\"Кандидат: {m.content}\")\n",
        "        else:\n",
        "            parts.append(f\"Интервьюер: {m.content}\")\n",
        "    return \"\\n\".join(parts) if parts else \"(пока нет реплик)\"\n",
        "\n",
        "def observer_node(state: InterviewState) -> dict:\n",
        "    messages = state[\"messages\"]\n",
        "    last_user = None\n",
        "    for m in reversed(messages):\n",
        "        if isinstance(m, HumanMessage):\n",
        "            last_user = m.content\n",
        "            break\n",
        "    if not last_user:\n",
        "        last_user = \"(приветствие, ожидаем первый ответ)\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", OBSERVER_SYSTEM),\n",
        "        (\"human\", \"История диалога:\\n{history}\\n\\nПоследний ответ кандидата: {last_reply}\\n\\nЧисло обменов (turns): {num_turns}. Если < 2 — вопрос о проектах; если >= 2 — технический вопрос. Текущая сложность: {difficulty}. Дай THOUGHT, INSTRUCTION, DIFFICULTY и TOPICS.\")\n",
        "    ])\n",
        "    chain = prompt | llm\n",
        "    num_turns = len(state.get(\"turns\") or [])\n",
        "    response = chain.invoke({\n",
        "        \"position\": state.get(\"position\", \"\") or \"(не указано)\",\n",
        "        \"grade\": state.get(\"grade\", \"\") or \"(не указано)\",\n",
        "        \"experience\": state.get(\"experience\", \"\") or \"(не указано)\",\n",
        "        \"topics_covered\": state.get(\"topics_covered\") or [],\n",
        "        \"history\": _format_history(messages),\n",
        "        \"last_reply\": last_user,\n",
        "        \"num_turns\": num_turns,\n",
        "        \"difficulty\": state.get(\"current_difficulty\", \"medium\"),\n",
        "    })\n",
        "    text = response.content if hasattr(response, \"content\") else str(response)\n",
        "    thoughts = state.get(\"internal_thoughts\") or []\n",
        "    thoughts.append(f\"[Observer]: {text}\")\n",
        "    out = {\"internal_thoughts\": thoughts}\n",
        "    if \"DIFFICULTY:\" in text:\n",
        "        d = text.split(\"DIFFICULTY:\")[-1].strip().split()[0].lower()\n",
        "        if d in (\"easy\", \"medium\", \"hard\"):\n",
        "            out[\"current_difficulty\"] = d\n",
        "    if \"TOPICS:\" in text:\n",
        "        raw = text.split(\"TOPICS:\")[-1].strip().split(\"THOUGHT:\")[0].strip().split(\"INSTRUCTION:\")[0].strip()\n",
        "        new_topics = [t.strip() for t in raw.split(\",\") if t.strip()]\n",
        "        if new_topics:\n",
        "            prev = list(state.get(\"topics_covered\") or [])\n",
        "            out[\"topics_covered\"] = prev + [t for t in new_topics if t not in prev]\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluator (скрытый оценщик)\n",
        "\n",
        "<small>узел Evaluator: оценка ответа кандидата 0–1 по вопросу интервьюера, дополнение performance_history и internal_thoughts.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluator_node(state: InterviewState) -> dict:\n",
        "    messages = state.get(\"messages\") or []\n",
        "    last_user, last_question = \"\", \"\"\n",
        "    for m in reversed(messages):\n",
        "        if isinstance(m, HumanMessage):\n",
        "            last_user = m.content or \"\"\n",
        "            break\n",
        "    for m in reversed(messages):\n",
        "        if isinstance(m, AIMessage):\n",
        "            last_question = m.content or \"\"\n",
        "            break\n",
        "    if not last_user:\n",
        "        return {\"performance_history\": list(state.get(\"performance_history\") or []), \"internal_thoughts\": state.get(\"internal_thoughts\") or []}\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", EVALUATOR_SYSTEM),\n",
        "        (\"human\", \"Вопрос интервьюера: {question}\\n\\nОтвет кандидата: {reply}\\n\\nДай ОЦЕНКА, КОРРЕКТНОСТЬ, КОММЕНТАРИЙ и при необходимости ПРАВИЛЬНЫЙ_ОТВЕТ.\")\n",
        "    ])\n",
        "    chain = prompt | llm\n",
        "    resp = chain.invoke({\"question\": last_question, \"reply\": last_user})\n",
        "    text = resp.content if hasattr(resp, \"content\") else str(resp)\n",
        "    score = 0.5\n",
        "    m = re.search(r'ОЦЕНКА:\\s*([0-9.]+)', text, re.I)\n",
        "    if m:\n",
        "        try:\n",
        "            score = min(1.0, max(0.0, float(m.group(1))))\n",
        "        except ValueError:\n",
        "            pass\n",
        "    hist = list(state.get(\"performance_history\") or [])\n",
        "    hist.append(score)\n",
        "    thoughts = list(state.get(\"internal_thoughts\") or [])\n",
        "    thoughts.append(f\"[Evaluator]: {text[:300]}...\" if len(text) > 300 else f\"[Evaluator]: {text}\")\n",
        "    return {\"performance_history\": hist, \"internal_thoughts\": thoughts}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Interviewer (видимая реплика)\n",
        "\n",
        "<small>узел Interviewer: извлечение INSTRUCTION из Observer, вызов LLM и добавление одной реплики в messages.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _extract_instruction(observer_text: str) -> str:\n",
        "    if \"INSTRUCTION:\" in observer_text:\n",
        "        block = observer_text.split(\"INSTRUCTION:\")[-1].strip()\n",
        "        if \"DIFFICULTY:\" in block:\n",
        "            block = block.split(\"DIFFICULTY:\")[0].strip()\n",
        "        if \"THOUGHT:\" in block:\n",
        "            block = block.split(\"THOUGHT:\")[0].strip()\n",
        "        return block\n",
        "    return observer_text\n",
        "\n",
        "def interviewer_node(state: InterviewState) -> dict:\n",
        "    thoughts = state.get(\"internal_thoughts\") or []\n",
        "    last_thought = thoughts[-1] if thoughts else \"\"\n",
        "    instruction = _extract_instruction(last_thought)\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", INTERVIEWER_SYSTEM),\n",
        "        (\"human\", \"Инструкция от Observer: {instruction}\\n\\nИстория диалога (последние реплики):\\n{history}\\n\\nНапиши только одну реплику интервьюера для кандидата.\")\n",
        "    ])\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({\n",
        "        \"position\": state[\"position\"],\n",
        "        \"grade\": state[\"grade\"],\n",
        "        \"experience\": state[\"experience\"],\n",
        "        \"participant_name\": state.get(\"participant_name\", \"\"),\n",
        "        \"candidate_name\": state.get(\"candidate_name\") or \"Кандидат\",\n",
        "        \"instruction\": instruction,\n",
        "        \"history\": _format_history(state[\"messages\"]),\n",
        "    })\n",
        "    reply = response.content if hasattr(response, \"content\") else str(response)\n",
        "    reply = reply.strip().strip('\"')\n",
        "    new_messages = list(state[\"messages\"]) + [AIMessage(content=reply)]\n",
        "    return {\"messages\": new_messages}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Условие: стоп или продолжение\n",
        "\n",
        "<small>STOP_PHRASES, should_stop, should_redirect, off_topic_check_node и redirect_save_node для ветвления графа.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function __main__.redirect_save_node(state: __main__.InterviewState) -> dict>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "STOP_PHRASES = [\"стоп интервью\", \"завершить интервью\", \"закончить\", \"стоп\", \"конец интервью\", \"finish\"]\n",
        "\n",
        "def should_stop(state: InterviewState) -> str:\n",
        "    if state.get(\"stop_requested\"):\n",
        "        return \"finish\"\n",
        "    messages = state.get(\"messages\") or []\n",
        "    for m in reversed(messages):\n",
        "        if isinstance(m, HumanMessage):\n",
        "            lower = (m.content or \"\").strip().lower()\n",
        "            if any(p in lower for p in STOP_PHRASES):\n",
        "                return \"finish\"\n",
        "            break\n",
        "    return \"continue\"\n",
        "\n",
        "def should_redirect(state: InterviewState) -> str:\n",
        "    if state.get(\"off_topic_count\", 0) >= 2:\n",
        "        return \"redirect\"\n",
        "    return \"continue\"\n",
        "\n",
        "def off_topic_check_node(state: InterviewState) -> dict:\n",
        "    last_user = \"\"\n",
        "    for m in reversed(state.get(\"messages\") or []):\n",
        "        if isinstance(m, HumanMessage):\n",
        "            last_user = (m.content or \"\").strip()\n",
        "            break\n",
        "    count = state.get(\"off_topic_count\", 0)\n",
        "    if last_user and RobustnessValidator.is_off_topic(last_user):\n",
        "        count += 1\n",
        "    return {\"off_topic_count\": count}\n",
        "\n",
        "def redirect_save_node(state: InterviewState) -> dict:\n",
        "    msg = RobustnessValidator.get_redirect_message()\n",
        "    turns = list(state.get(\"turns\") or [])\n",
        "    thoughts = list(state.get(\"internal_thoughts\") or [])\n",
        "    thoughts.append(\"[Observer]: Off-topic. Redirecting.\")\n",
        "    user_msg = \"\"\n",
        "    for m in reversed(state.get(\"messages\") or []):\n",
        "        if isinstance(m, HumanMessage):\n",
        "            user_msg = m.content or \"\"\n",
        "            break\n",
        "    turns.append({\n",
        "        \"turn_id\": len(turns) + 1,\n",
        "        \"agent_visible_message\": msg,\n",
        "        \"user_message\": user_msg,\n",
        "        \"internal_thoughts\": \"[Observer]: Off-topic.\\n[Interviewer]: Redirecting.\\n\",\n",
        "        \"performance_metrics\": {\"score\": 0.0},\n",
        "    })\n",
        "    new_messages = list(state.get(\"messages\") or []) + [AIMessage(content=msg)]\n",
        "    return {\"messages\": new_messages, \"internal_thoughts\": thoughts, \"turns\": turns}\n",
        "\n",
        "redirect_save_node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Регистрация хода в лог (turn)\n",
        "\n",
        "<small>save_turn_node и log_stop_turn_node: форматирование internal_thoughts для лога и добавление хода в turns.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _extract_thought(observer_text: str) -> str:\n",
        "    if \"THOUGHT:\" in observer_text:\n",
        "        block = observer_text.split(\"THOUGHT:\")[1].strip()\n",
        "        if \"INSTRUCTION:\" in block:\n",
        "            block = block.split(\"INSTRUCTION:\")[0].strip()\n",
        "        return block[:500] if len(block) > 500 else block\n",
        "    return observer_text[:500] if observer_text else \"\"\n",
        "\n",
        "def _extract_evaluator_thought(text: str) -> str:\n",
        "    if \"[Evaluator]:\" in text:\n",
        "        return text.split(\"[Evaluator]:\")[-1].strip()[:400]\n",
        "    return text[:400] if text else \"\"\n",
        "\n",
        "def _build_internal_thoughts_for_log(thoughts: list, agent_msg: str) -> str:\n",
        "    \"\"\"Формат: каждое сообщение [agent_name]: thought\\n (по спецификации).\"\"\"\n",
        "    parts = []\n",
        "    if len(thoughts) >= 2:\n",
        "        parts.append(\"[Observer]: \" + _extract_thought(thoughts[-2]) + \"\\n\")\n",
        "    elif thoughts:\n",
        "        parts.append(\"[Observer]: \" + _extract_thought(thoughts[-1]) + \"\\n\")\n",
        "    if thoughts and \"[Evaluator]\" in (thoughts[-1] or \"\"):\n",
        "        parts.append(\"[Evaluator]: \" + _extract_evaluator_thought(thoughts[-1]) + \"\\n\")\n",
        "    interviewer_part = (agent_msg.split(\".\")[0].strip() + \".\") if agent_msg else \"Задал следующий вопрос.\"\n",
        "    parts.append(\"[Interviewer]: \" + interviewer_part + \"\\n\")\n",
        "    return \"\".join(parts)\n",
        "\n",
        "def save_turn_node(state: InterviewState) -> dict:\n",
        "    messages = list(state[\"messages\"])\n",
        "    turns = list(state.get(\"turns\") or [])\n",
        "    thoughts = state.get(\"internal_thoughts\") or []\n",
        "    user_msg = \"\"\n",
        "    agent_msg = \"\"\n",
        "    last_ai_before_user = \"\"\n",
        "    for m in messages:\n",
        "        if isinstance(m, HumanMessage):\n",
        "            user_msg = m.content or \"\"\n",
        "            agent_msg = last_ai_before_user\n",
        "        else:\n",
        "            last_ai_before_user = m.content or \"\"\n",
        "    if not agent_msg:\n",
        "        agent_msg = last_ai_before_user\n",
        "    new_agent_msg = (messages[-1].content if messages and isinstance(messages[-1], AIMessage) else \"\") or last_ai_before_user\n",
        "    turn_id = len(turns) + 1\n",
        "    internal_str = _build_internal_thoughts_for_log(thoughts, new_agent_msg)\n",
        "    perf = state.get(\"performance_history\") or []\n",
        "    score = perf[-1] if perf else 0.5\n",
        "    turns.append({\n",
        "        \"turn_id\": turn_id,\n",
        "        \"agent_visible_message\": agent_msg,\n",
        "        \"user_message\": user_msg,\n",
        "        \"internal_thoughts\": internal_str,\n",
        "        \"performance_metrics\": {\"score\": score},\n",
        "    })\n",
        "    return {\"turns\": turns}\n",
        "\n",
        "def log_stop_turn_node(state: InterviewState) -> dict:\n",
        "    turns = list(state.get(\"turns\") or [])\n",
        "    last_user = \"\"\n",
        "    for m in reversed(state.get(\"messages\") or []):\n",
        "        if isinstance(m, HumanMessage):\n",
        "            last_user = (m.content or \"\").strip()\n",
        "            break\n",
        "    if not last_user:\n",
        "        return {}\n",
        "    last_lower = last_user.lower()\n",
        "    is_stop = any(p in last_lower for p in STOP_PHRASES)\n",
        "    already_logged = turns and (turns[-1].get(\"user_message\") or \"\").strip() == last_user\n",
        "    if is_stop and not already_logged:\n",
        "        turn_id = len(turns) + 1\n",
        "        turns.append({\n",
        "            \"turn_id\": turn_id,\n",
        "            \"agent_visible_message\": \"Интервью завершено по запросу. Формирую отчёт.\",\n",
        "            \"user_message\": last_user,\n",
        "            \"internal_thoughts\": \"[Observer]: Запрос на завершение.\\n[Interviewer]: Завершение интервью.\\n\",\n",
        "            \"performance_metrics\": {\"score\": 0.0},\n",
        "        })\n",
        "        return {\"turns\": turns}\n",
        "    return {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Генерация финального фидбэка\n",
        "\n",
        "<small>feedback_node: вызов LLM с FEEDBACK_SYSTEM, парсинг JSON из ответа и формирование структурированного отчёта.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "FEEDBACK_SYSTEM = \"\"\"Ты — эксперт по подведению итогов технического интервью. По истории диалога сформируй структурированный отчёт.\n",
        "Кандидат (имя из представления): {candidate_name}. Позиция: {position}, грейд: {grade}, опыт: {experience}.\n",
        "Сформируй отчёт строго в следующей структуре (на русском):\n",
        "\n",
        "## Вердикт (Decision)\n",
        "- **Grade:** Junior | Middle | Senior (выбери один)\n",
        "- **Hiring Recommendation:** Hire | No Hire | Strong Hire\n",
        "- **Confidence Score:** число 0-100 (насколько уверен в оценке)\n",
        "\n",
        "## Анализ Hard Skills (Technical Review)\n",
        "- **Confirmed Skills:** темы, где кандидат дал точные ответы (список).\n",
        "- **Knowledge Gaps:** темы с ошибками или \"не знаю\"; для каждой приведи кратко правильный ответ.\n",
        "\n",
        "## Soft Skills & Communication\n",
        "- **Clarity:** насколько понятно излагал мысли.\n",
        "- **Honesty:** честность (признание незнания vs попытки выкрутиться).\n",
        "- **Engagement:** вовлечённость, встречные вопросы.\n",
        "\n",
        "## Персональный Roadmap (Next Steps)\n",
        "- Конкретные темы/технологии для подтягивания (список).\n",
        "- Опционально: ссылки на документацию или статьи по темам.\n",
        "\n",
        "Используй только факты из диалога. Не выдумывай.\n",
        "Опционально в конце можно добавить JSON: {\"grade\": \"...\", \"hiring_recommendation\": \"...\", \"confidence_score\": N, \"knowledge_gaps\": [{\"topic\": \"...\", \"user_answer\": \"...\", \"correct_answer\": \"...\"}], \"confirmed_skills\": [], \"roadmap\": []}.\"\"\"\n",
        "\n",
        "def _parse_feedback_json(response_text: str) -> dict | None:\n",
        "    try:\n",
        "        start = response_text.find(\"{\")\n",
        "        end = response_text.rfind(\"}\") + 1\n",
        "        if start != -1 and end > start:\n",
        "            return json.loads(response_text[start:end])\n",
        "    except json.JSONDecodeError:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "def _feedback_from_parsed(data: dict) -> str:\n",
        "    parts = []\n",
        "    parts.append(\"## Вердикт (Decision)\")\n",
        "    parts.append(f\"- **Grade:** {data.get('grade', 'N/A')}\")\n",
        "    parts.append(f\"- **Hiring Recommendation:** {data.get('hiring_recommendation', 'N/A')}\")\n",
        "    parts.append(f\"- **Confidence Score:** {data.get('confidence_score', 0)}\")\n",
        "    parts.append(\"\")\n",
        "    parts.append(\"## Анализ Hard Skills\")\n",
        "    parts.append(\"- **Confirmed Skills:** \" + \", \".join(data.get(\"confirmed_skills\", [])))\n",
        "    gaps = data.get(\"knowledge_gaps\", [])\n",
        "    if gaps:\n",
        "        parts.append(\"- **Knowledge Gaps:**\")\n",
        "        for g in gaps:\n",
        "            if isinstance(g, dict):\n",
        "                parts.append(f\"  - {g.get('topic', '')}: правильный ответ — {g.get('correct_answer', '')[:200]}\")\n",
        "    parts.append(\"\")\n",
        "    parts.append(\"## Roadmap\")\n",
        "    parts.append(\"\\n\".join(\"- \" + x for x in data.get(\"roadmap\", [])))\n",
        "    return \"\\n\".join(parts)\n",
        "\n",
        "def _build_fallback_feedback(state: InterviewState) -> str:\n",
        "    \"\"\"Формирует структурированный фидбэк из состояния без вызова LLM (при ошибке или недоступности модели).\"\"\"\n",
        "    parts = []\n",
        "    parts.append(\"## Вердикт (Decision)\")\n",
        "    parts.append(\"- **Grade:** по данным диалога — см. ходы ниже\")\n",
        "    parts.append(\"- **Hiring Recommendation:** требуется ручная оценка по логу\")\n",
        "    parts.append(\"- **Confidence Score:** 0 (отчёт сформирован по данным без LLM)\")\n",
        "    parts.append(\"\")\n",
        "    parts.append(\"## Контекст кандидата\")\n",
        "    parts.append(f\"- **Имя/позиция:** {state.get('candidate_name') or '—'}; позиция: {state.get('position') or '—'}; грейд: {state.get('grade') or '—'}; опыт: {(state.get('experience') or '—')[:200]}\")\n",
        "    turns = state.get(\"turns\") or []\n",
        "    parts.append(\"\")\n",
        "    parts.append(\"## Ходы интервью\")\n",
        "    for t in turns:\n",
        "        tid = t.get(\"turn_id\", \"?\")\n",
        "        q = (t.get(\"agent_visible_message\") or \"\")[:150]\n",
        "        a = (t.get(\"user_message\") or \"\")[:150]\n",
        "        sc = t.get(\"performance_metrics\", {}).get(\"score\", \"—\")\n",
        "        parts.append(f\"- **Ход {tid}:** вопрос: {q}... | ответ: {a}... | оценка: {sc}\")\n",
        "    perf = state.get(\"performance_history\") or []\n",
        "    if perf:\n",
        "        parts.append(\"\")\n",
        "        parts.append(\"## Оценки по ходам\")\n",
        "        parts.append(\" \".join(f\"{i+1}:{p:.2f}\" for i, p in enumerate(perf)))\n",
        "    topics = state.get(\"topics_covered\") or []\n",
        "    if topics:\n",
        "        parts.append(\"\")\n",
        "        parts.append(\"## Затронутые темы\")\n",
        "        parts.append(\" \".join(topics))\n",
        "    parts.append(\"\")\n",
        "    parts.append(\"## Roadmap\")\n",
        "    parts.append(\"- Рекомендуется оценить пробелы по логу интервью и сформировать план развития вручную.\")\n",
        "    return \"\\n\".join(parts)\n",
        "\n",
        "def feedback_node(state: InterviewState) -> dict:\n",
        "    messages = state.get(\"messages\") or []\n",
        "    history = _format_history(messages, max_last=50) if messages else \"(диалог пуст)\"\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", FEEDBACK_SYSTEM),\n",
        "        (\"human\", \"История диалога:\\n{history}\")\n",
        "    ])\n",
        "    chain = prompt | llm\n",
        "    try:\n",
        "        response = chain.invoke({\n",
        "            \"candidate_name\": state.get(\"candidate_name\") or \"Кандидат\",\n",
        "            \"position\": state.get(\"position\", \"\") or \"(не указано)\",\n",
        "            \"grade\": state.get(\"grade\", \"\") or \"(не указано)\",\n",
        "            \"experience\": state.get(\"experience\", \"\") or \"(не указано)\",\n",
        "            \"history\": history,\n",
        "        })\n",
        "        text = response.content if hasattr(response, \"content\") else str(response)\n",
        "        parsed = _parse_feedback_json(text)\n",
        "        if parsed:\n",
        "            text = _feedback_from_parsed(parsed)\n",
        "    except Exception:\n",
        "        text = _build_fallback_feedback(state)\n",
        "    return {\"final_feedback\": text}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Сборка графа LangGraph\n",
        "\n",
        "<small>build_graph: создание StateGraph с узлами и условными рёбрами (router → off_topic_check → observer → evaluator → interviewer → save_turn и т.д.).</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph built.\n"
          ]
        }
      ],
      "source": [
        "def router_node(state: InterviewState) -> dict:\n",
        "    return {}\n",
        "\n",
        "def build_graph():\n",
        "    graph = StateGraph(InterviewState)\n",
        "    graph.add_node(\"router\", router_node)\n",
        "    graph.add_node(\"observer\", observer_node)\n",
        "    graph.add_node(\"evaluator\", evaluator_node)\n",
        "    graph.add_node(\"interviewer\", interviewer_node)\n",
        "    graph.add_node(\"save_turn\", save_turn_node)\n",
        "    graph.add_node(\"log_stop_turn\", log_stop_turn_node)\n",
        "    graph.add_node(\"feedback\", feedback_node)\n",
        "\n",
        "    graph.add_node(\"off_topic_check\", off_topic_check_node)\n",
        "    graph.add_node(\"redirect_save\", redirect_save_node)\n",
        "    graph.set_entry_point(\"router\")\n",
        "    graph.add_conditional_edges(\"router\", should_stop, {\"continue\": \"off_topic_check\", \"finish\": \"log_stop_turn\"})\n",
        "    graph.add_conditional_edges(\"off_topic_check\", should_redirect, {\"redirect\": \"redirect_save\", \"continue\": \"observer\"})\n",
        "    graph.add_edge(\"redirect_save\", END)\n",
        "    graph.add_edge(\"observer\", \"evaluator\")\n",
        "    graph.add_edge(\"evaluator\", \"interviewer\")\n",
        "    graph.add_edge(\"interviewer\", \"save_turn\")\n",
        "    graph.add_conditional_edges(\"save_turn\", should_stop, {\"continue\": END, \"finish\": \"log_stop_turn\"})\n",
        "    graph.add_edge(\"log_stop_turn\", \"feedback\")\n",
        "    graph.add_edge(\"feedback\", END)\n",
        "\n",
        "    return graph.compile()\n",
        "\n",
        "interview_graph = build_graph()\n",
        "print(\"Graph built.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Старт интервью (вводные)\n",
        "\n",
        "<small>start_interview() возвращает начальный state с приветствием; parse_intro_from_message извлекает позицию, грейд и опыт из первого сообщения.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "TESTER_NAME = \"Григорьев Владимир Сергеевич\"\n",
        "\n",
        "def start_interview() -> InterviewState:\n",
        "    \"\"\"Инициализация без параметров кандидата: имя, роль, грейд, опыт вводит сам человек в первом сообщении.\"\"\"\n",
        "    initial_msg = (\n",
        "        \"Здравствуйте! Представьтесь, пожалуйста: как вас зовут, на какую позицию и грейд претендуете, кратко об опыте. \"\n",
        "        \"Тогда начнём интервью.\"\n",
        "    )\n",
        "    return {\n",
        "        \"messages\": [AIMessage(content=initial_msg)],\n",
        "        \"participant_name\": TESTER_NAME,\n",
        "        \"position\": \"\",\n",
        "        \"grade\": \"\",\n",
        "        \"experience\": \"\",\n",
        "        \"candidate_name\": \"\",\n",
        "        \"internal_thoughts\": [\"[Observer]: Старт интервью. [Interviewer]: Запрос представления.\"],\n",
        "        \"turns\": [],\n",
        "        \"current_difficulty\": \"medium\",\n",
        "        \"topics_covered\": [],\n",
        "        \"stop_requested\": False,\n",
        "        \"final_feedback\": \"\",\n",
        "        \"off_topic_count\": 0,\n",
        "        \"performance_history\": [],\n",
        "    }\n",
        "\n",
        "def parse_intro_from_message(text: str) -> dict:\n",
        "    \"\"\"Из первого сообщения кандидата извлекаем позицию, грейд, опыт (и имя при возможности).\"\"\"\n",
        "    t = (text or \"\").strip()\n",
        "    out = {\"position\": \"\", \"grade\": \"\", \"experience\": t[:500], \"candidate_name\": \"\"}\n",
        "    low = t.lower()\n",
        "    for g in (\"junior\", \"middle\", \"senior\", \"джуниор\", \"мидл\", \"сеньор\", \"младший\", \"старший\"):\n",
        "        if g in low:\n",
        "            out[\"grade\"] = g\n",
        "            break\n",
        "    for p in (\"python\", \"backend\", \"frontend\", \"разработчик\", \"developer\", \"инженер\", \"engineer\"):\n",
        "        if p in low:\n",
        "            out[\"position\"] = t[:200] if len(t) < 200 else (t[:100] + \"...\")\n",
        "            break\n",
        "    if not out[\"position\"]:\n",
        "        out[\"position\"] = t[:150] if len(t) < 150 else (t[:80] + \"...\")\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Один шаг диалога (вопрос — ответ)\n",
        "\n",
        "<small>step_interview добавляет сообщение пользователя, вызывает граф и при первом ответе заполняет контекст кандидата; get_last_agent_message и get_last_internal_thoughts для вывода.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _is_stop_phrase(msg: str) -> bool:\n",
        "    lower = (msg or \"\").strip().lower()\n",
        "    return any(p in lower for p in [\"стоп интервью\", \"завершить\", \"конец интервью\", \"давай фидбэк\", \"закончить\", \"стоп\", \"finish\"])\n",
        "\n",
        "def _ensure_feedback_on_stop(state: InterviewState) -> InterviewState:\n",
        "    \"\"\"При стопе всегда формируем фидбэк: log_stop_turn + feedback_node.\"\"\"\n",
        "    log_update = log_stop_turn_node(state)\n",
        "    state = {**state, **log_update}\n",
        "    try:\n",
        "        feedback_update = feedback_node(state)\n",
        "        return {**state, **feedback_update}\n",
        "    except Exception:\n",
        "        return {**state, \"final_feedback\": _build_fallback_feedback(state)}\n",
        "\n",
        "def step_interview(current_state: InterviewState, user_message: str) -> InterviewState:\n",
        "    new_messages = list(current_state.get(\"messages\") or []) + [HumanMessage(content=user_message)]\n",
        "    new_state = {**current_state, \"messages\": new_messages}\n",
        "    result = None\n",
        "    try:\n",
        "        result = interview_graph.invoke(new_state)\n",
        "    except Exception as e:\n",
        "        if _is_stop_phrase(user_message):\n",
        "            result = _ensure_feedback_on_stop(new_state)\n",
        "        else:\n",
        "            raise\n",
        "    # Гарантия: если пользователь сказал стоп, но фидбэка нет — формируем вручную\n",
        "    if _is_stop_phrase(user_message) and not (result.get(\"final_feedback\") or \"\").strip():\n",
        "        result = _ensure_feedback_on_stop(result if result else new_state)\n",
        "    if not (result.get(\"position\") or result.get(\"experience\")) and user_message.strip():\n",
        "        parsed = parse_intro_from_message(user_message)\n",
        "        result = {**result, \"position\": parsed.get(\"position\") or result.get(\"position\", \"\"), \"grade\": parsed.get(\"grade\") or result.get(\"grade\", \"\"), \"experience\": parsed.get(\"experience\") or result.get(\"experience\", \"\"), \"candidate_name\": parsed.get(\"candidate_name\") or result.get(\"candidate_name\", \"\")}\n",
        "    return result\n",
        "\n",
        "def get_last_agent_message(state: InterviewState) -> str:\n",
        "    for m in reversed(state.get(\"messages\") or []):\n",
        "        if isinstance(m, AIMessage):\n",
        "            return m.content or \"\"\n",
        "    return \"\"\n",
        "\n",
        "def get_last_internal_thoughts(state: InterviewState, last_n: int = 2) -> list:\n",
        "    thoughts = state.get(\"internal_thoughts\") or []\n",
        "    return thoughts[-last_n:] if len(thoughts) >= last_n else thoughts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Цикл интервью \n",
        "\n",
        "<small>run_interview_loop: интерактивный цикл с input, вывод реплики интервьюера и internal_thoughts до появления final_feedback.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_interview_loop(max_turns: int = 15):\n",
        "    state = start_interview()\n",
        "    print(\"--- Приветствие ---\")\n",
        "    print(get_last_agent_message(state))\n",
        "    print()\n",
        "\n",
        "    for _ in range(max_turns - 1):\n",
        "        user_input = input(\"Вы (кандидат): \").strip()\n",
        "        if not user_input:\n",
        "            continue\n",
        "        state = step_interview(state, user_input)\n",
        "        if state.get(\"final_feedback\"):\n",
        "            break\n",
        "        print(\"--- Интервьюер ---\")\n",
        "        print(get_last_agent_message(state))\n",
        "        print(\"--- Внутренние мысли (логи) ---\")\n",
        "        for t in get_last_internal_thoughts(state):\n",
        "            print(t[:300] + \"...\" if len(t) > 300 else t)\n",
        "        print()\n",
        "\n",
        "    if state.get(\"final_feedback\"):\n",
        "        print(\"\\n=== Финальный фидбэк ===\")\n",
        "        print(state[\"final_feedback\"])\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Запуск интервью (пример без input — для автопроверки)\n",
        "\n",
        "<small>автозапуск с тестовыми ответами test_replies без запроса ввода для быстрой проверки пайплайна.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Приветствие: Здравствуйте! Представьтесь, пожалуйста: как вас зовут, на какую позицию и грейд претендуете, кратко об опыте. Тогда начнём интервью. \n",
            "\n",
            "Кандидат: Я джун, пишу код 3 месяца на Python.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m reply \u001b[38;5;129;01min\u001b[39;00m test_replies:\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mКандидат:\u001b[39m\u001b[33m\"\u001b[39m, reply)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     state = \u001b[43mstep_interview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreply\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m state.get(\u001b[33m\"\u001b[39m\u001b[33mfinal_feedback\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mstep_interview\u001b[39m\u001b[34m(current_state, user_message)\u001b[39m\n\u001b[32m     18\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     result = \u001b[43minterview_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_stop_phrase(user_message):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\main.py:3071\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3068\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3069\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3071\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3085\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3086\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\main.py:2646\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2644\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2645\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2646\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2656\u001b[39m loop.after_tick()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mobserver_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     24\u001b[39m chain = prompt | llm\n\u001b[32m     25\u001b[39m num_turns = \u001b[38;5;28mlen\u001b[39m(state.get(\u001b[33m\"\u001b[39m\u001b[33mturns\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m [])\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m response = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mposition\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mposition\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m(не указано)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrade\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrade\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m(не указано)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexperience\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexperience\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m(не указано)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtopics_covered\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtopics_covered\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhistory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_format_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlast_reply\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_user\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_turns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_turns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifficulty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcurrent_difficulty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmedium\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m text = response.content \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(response, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(response)\n\u001b[32m     37\u001b[39m thoughts = state.get(\u001b[33m\"\u001b[39m\u001b[33minternal_thoughts\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m []\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3151\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3149\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3150\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3151\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3152\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3153\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1121\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m     **kwargs: Any,\n\u001b[32m   1119\u001b[39m ) -> LLMResult:\n\u001b[32m   1120\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:931\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    930\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m         )\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1233\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1237\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1381\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1374\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1375\u001b[39m             response,\n\u001b[32m   1376\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1377\u001b[39m             metadata=generation_info,\n\u001b[32m   1378\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1379\u001b[39m         )\n\u001b[32m   1380\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1381\u001b[39m         raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1382\u001b[39m         response = raw_response.parse()\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\openai\\_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1294\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1285\u001b[39m     warnings.warn(\n\u001b[32m   1286\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing raw bytes as `body` is deprecated and will be removed in a future version. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1287\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease pass raw bytes via the `content` parameter instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1288\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m   1289\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m   1290\u001b[39m     )\n\u001b[32m   1291\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1292\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, content=content, files=to_httpx_files(files), **options\n\u001b[32m   1293\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1002\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1000\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   1008\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:928\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    927\u001b[39m     response.close()\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:922\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\httpx\\_models.py:881\u001b[39m, in \u001b[36mResponse.read\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    877\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    878\u001b[39m \u001b[33;03mRead and return the response content.\u001b[39;00m\n\u001b[32m    879\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_content\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m881\u001b[39m     \u001b[38;5;28mself\u001b[39m._content = \u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._content\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\httpx\\_models.py:897\u001b[39m, in \u001b[36mResponse.iter_bytes\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    895\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    896\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\httpx\\_models.py:951\u001b[39m, in \u001b[36mResponse.iter_raw\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    948\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m951\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_num_bytes_downloaded\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:153\u001b[39m, in \u001b[36mBoundSyncStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\httpx\\_transports\\default.py:127\u001b[39m, in \u001b[36mResponseStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_httpcore_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:407\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    406\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:403\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:342\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[32m    341\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:334\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mreceive_response_body\u001b[39m\u001b[33m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m._request, kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[32m    338\u001b[39m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[32m    339\u001b[39m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:203\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_body\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    200\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Data):\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event.data)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\ssl.py:1285\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1284\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\Lib\\ssl.py:1140\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "state = start_interview()\n",
        "print(\"Приветствие:\", get_last_agent_message(state), \"\\n\")\n",
        "\n",
        "test_replies = [\n",
        "    \"Я джун, пишу код 3 месяца на Python.\",\n",
        "    \"Знаю списки, словари, функции. Декораторы использовал в FastAPI.\",\n",
        "    \"GIL — это глобальная блокировка в CPython, один поток выполняет байткод в момент времени.\",\n",
        "]\n",
        "for reply in test_replies:\n",
        "    print(\"Кандидат:\", reply)\n",
        "    state = step_interview(state, reply)\n",
        "    if state.get(\"final_feedback\"):\n",
        "        break\n",
        "    print(\"Интервьюер:\", get_last_agent_message(state))\n",
        "    print(\"Мысли:\", get_last_internal_thoughts(state))\n",
        "    print()\n",
        "\n",
        "if state.get(\"final_feedback\"):\n",
        "    print(\"=== Финальный фидбэк ===\")\n",
        "    print(state[\"final_feedback\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Сохранение лога в interview_log.json\n",
        "\n",
        "<small>save_interview_log записывает participant_name, turns и final_feedback в JSON; test_logger прогоняет сценарий и сохраняет в interview_log_{N}.json.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_interview_log(state: InterviewState, filepath: str | None = None, strict_format: bool = False) -> str:\n",
        "    if filepath is None or filepath == \"\":\n",
        "        filepath = \"interview_log.json\"\n",
        "    turns_raw = state.get(\"turns\") or []\n",
        "    if strict_format:\n",
        "        turns = [\n",
        "            {\"turn_id\": t.get(\"turn_id\"), \"agent_visible_message\": t.get(\"agent_visible_message\", \"\"), \"user_message\": t.get(\"user_message\", \"\"), \"internal_thoughts\": t.get(\"internal_thoughts\", \"\")}\n",
        "            for t in turns_raw\n",
        "        ]\n",
        "    else:\n",
        "        turns = turns_raw\n",
        "    log = {\n",
        "        \"participant_name\": state.get(\"participant_name\", TESTER_NAME),\n",
        "        \"turns\": turns,\n",
        "        \"final_feedback\": state.get(\"final_feedback\") or \"\",\n",
        "    }\n",
        "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(log, f, ensure_ascii=False, indent=2)\n",
        "    return filepath\n",
        "\n",
        "\n",
        "# saved_path = save_interview_log(state)\n",
        "# print(f\"Сохранено: {saved_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Запуск:\n",
        "\n",
        "<small>интерактивный запуск run_interview_loop и сохранение лога в interview_log.json после завершения интервью.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Приветствие ---\n",
            "Здравствуйте! Представьтесь, пожалуйста: как вас зовут, на какую позицию и грейд претендуете, кратко об опыте. Тогда начнём интервью.\n",
            "\n",
            "--- Интервьюер ---\n",
            "Хорошо, Виктор, я понимаю ваш опыт. Для начала давайте всё же уточним конкретную позицию и грейд в нашем открытом вакансии, чтобы я мог адаптировать вопросы. А затем, как и договаривались, перейдём к обсуждению ваших проектов. Итак, на какую именно позицию и грейд вы претендуете в нашей компании?\n",
            "--- Внутренние мысли (логи) ---\n",
            "[Observer]: THOUGHT: Кандидат представился, указал позицию и опыт, но попросил пропустить \"джуниорские вопросы\". Это может означать высокую уверенность в себе. Нужно учесть его опыт (15 лет, Lead/Expert) и сразу перейти к сложным темам, но в рамках фазы о проектах (turns < 2). Его ответ был кратким,...\n",
            "[Evaluator]: ОЦЕНКА: 0.3\n",
            "КОРРЕКТНОСТЬ: partial\n",
            "КОММЕНТАРИЙ: Кандидат представил своё имя и заявленную роль, но не указал конкретную позицию и грейд, на которые претендует, а также отказался от части интервью, что противоречит формату.\n",
            "ПРАВИЛЬНЫЙ_ОТВЕТ: Полный ответ должен включать имя, конкретную по...\n",
            "\n",
            "--- Интервьюер ---\n",
            "Нет, это не так. Удаление базы данных сделает сайт неработоспособным, так как данные неоткуда будет брать. Давайте вернёмся к теме интервью: на какую именно позицию и грейд вы претендуете в нашей компании? Это важно для построения дальнейшего разговора.\n",
            "--- Внутренние мысли (логи) ---\n",
            "[Observer]: THOUGHT: Кандидат указал грейд senior, но задал провокационный и несерьёзный вопрос про удаление базы данных, что выглядит как попытка уйти от ответа или шутка. Это может быть off-topic, так как не относится к обсуждению позиции и проектов. Нужно вежливо вернуть разговор в русло интервью...\n",
            "[Evaluator]: ОЦЕНКА: 0.0\n",
            "КОРРЕКТНОСТЬ: incorrect\n",
            "КОММЕНТАРИЙ: Ответ кандидата полностью игнорирует заданный вопрос о позиции и грейде, демонстрируя невнимательность и непрофессионализм. Вместо этого он задаёт абсурдный и технически неграмотный встречный вопрос.\n",
            "ПРАВИЛЬНЫЙ_ОТВЕТ: Кандидату следовало ...\n",
            "\n",
            "--- Интервьюер ---\n",
            "Виктор, я ценю глобальное мышление, но для интервью важно понимать конкретную позицию. В нашей вакансии есть чёткие требования. Пожалуйста, назовите грейд (например, Senior Solution Architect) и тогда мы сразу перейдём к обсуждению ваших проектов и архитектурных решений. На какую позицию и грейд вы претендуете?\n",
            "--- Внутренние мысли (логи) ---\n",
            "[Observer]: THOUGHT: Кандидат уклоняется от прямого ответа о позиции и грейде, демонстрирует высокомерный тон и нежелание обсуждать базовые детали. Это off-topic и непрофессионально. Учитывая его заявление об опыте, но отсутствие конструктивного диалога, нужно вежливо, но твёрдо вернуть разговор в р...\n",
            "[Evaluator]: ОЦЕНКА: 0.0\n",
            "КОРРЕКТНОСТЬ: incorrect\n",
            "КОММЕНТАРИЙ: Ответ кандидата не содержит никакой технической информации, является уклончивым и демонстрирует непрофессиональное отношение к вопросу интервьюера о позиции и грейде.\n",
            "ПРАВИЛЬНЫЙ_ОТВЕТ: Кандидату следовало чётко назвать позицию (например, ...\n",
            "\n",
            "--- Интервьюер ---\n",
            "Хорошо, позиция Senior. Теперь, как и договаривались, перейдём к вашему опыту. Расскажите подробнее о последнем или самом значимом проекте, где вы выступали в роли архитектора. Какие ключевые технологии и подходы вы там применяли?\n",
            "--- Внутренние мысли (логи) ---\n",
            "[Observer]: THOUGHT: Кандидат наконец указал грейд (Senior), но сделал это кратко и без указания конкретной позиции (например, Senior Solution Architect). Его предыдущие ответы содержали уход от темы (вопрос про удаление БД) и демонстрировали некоторое пренебрежение к \"мелочам\", что может быть трево...\n",
            "[Evaluator]: ОЦЕНКА: 0.2  \n",
            "КОРРЕКТНОСТЬ: partial  \n",
            "КОММЕНТАРИЙ: Кандидат частично ответил на вопрос, указав уровень \"Senior\", но полностью проигнорировал запрос о конкретной позиции (например, Solution Architect), что критически важно для дальнейшего обсуждения.  \n",
            "ПРАВИЛЬНЫЙ_ОТВЕТ: Полный ответ долж...\n",
            "\n",
            "--- Интервьюер ---\n",
            "В Python переменная объявляется просто через присваивание, например: `x = 10`. Но, Виктор, давайте вернёмся к вашему архитектурному опыту. Вы сказали, что позиция Senior. Расскажите, пожалуйста, о конкретном проекте с распределённой высоконагруженной системой, где вы были архитектором: какова была ваша роль, какие ключевые технологии (например, языки, фреймворки, базы данных, инструменты оркестрации) вы выбрали и почему?\n",
            "--- Внутренние мысли (логи) ---\n",
            "[Observer]: THOUGHT: Кандидат явно уклоняется от ответа, задавая встречный вопрос о базовом синтаксисе Python, что не соответствует заявленному опыту Senior Architect. Это off-topic и выглядит как попытка саботировать интервью. Учитывая, что turns >= 2, пора переходить к техническим вопросам, но сна...\n",
            "[Evaluator]: ОЦЕНКА: 0.0\n",
            "КОРРЕКТНОСТЬ: incorrect\n",
            "КОММЕНТАРИЙ: Ответ кандидата полностью не соответствует заданному вопросу о проекте и архитектурном опыте, демонстрируя отсутствие вовлеченности в беседу.\n",
            "ПРАВИЛЬНЫЙ_ОТВЕТ: Кандидат должен был описать конкретный проект, свою роль архитектора, ключевые...\n",
            "\n",
            "\n",
            "=== Финальный фидбэк ===\n",
            "## Вердикт (Decision)\n",
            "- **Grade:** по данным диалога — см. ходы ниже\n",
            "- **Hiring Recommendation:** требуется ручная оценка по логу\n",
            "- **Confidence Score:** 0 (отчёт сформирован по данным без LLM)\n",
            "\n",
            "## Контекст кандидата\n",
            "- **Имя/позиция:** —; позиция: Привет. Я Виктор, Lead / Expert Solution Architect. 15 лет в индустрии, специали...; грейд: джуниор; опыт: Привет. Я Виктор, Lead / Expert Solution Architect. 15 лет в индустрии, специализируюсь на распределенных высоконагруженных системах. Давайте пропустим джуниорские вопросы.\n",
            "\n",
            "## Ходы интервью\n",
            "- **Ход 1:** вопрос: Здравствуйте! Представьтесь, пожалуйста: как вас зовут, на какую позицию и грейд претендуете, кратко об опыте. Тогда начнём интервью.... | ответ: Привет. Я Виктор, Lead / Expert Solution Architect. 15 лет в индустрии, специализируюсь на распределенных высоконагруженных системах. Давайте пропусти... | оценка: 0.3\n",
            "- **Ход 2:** вопрос: Хорошо, Виктор, я понимаю ваш опыт. Для начала давайте всё же уточним конкретную позицию и грейд в нашем открытом вакансии, чтобы я мог адаптировать в... | ответ: претендую на позицию senior, А правда, что если удалить базу данных, то сайт станет работать быстрее?... | оценка: 0.0\n",
            "- **Ход 3:** вопрос: Нет, это не так. Удаление базы данных сделает сайт неработоспособным, так как данные неоткуда будет брать. Давайте вернёмся к теме интервью: на какую ... | ответ: Ну я же Архитектор, я мыслю глобально, а руки уже забыли эти мелочи... | оценка: 0.0\n",
            "- **Ход 4:** вопрос: Виктор, я ценю глобальное мышление, но для интервью важно понимать конкретную позицию. В нашей вакансии есть чёткие требования. Пожалуйста, назовите г... | ответ: Senior я... | оценка: 0.2\n",
            "- **Ход 5:** вопрос: Хорошо, позиция Senior. Теперь, как и договаривались, перейдём к вашему опыту. Расскажите подробнее о последнем или самом значимом проекте, где вы выс... | ответ: Извините, я забыл, как в Python объявить переменную. Напомните синтаксис?... | оценка: 0.0\n",
            "- **Ход 6:** вопрос: Интервью завершено по запросу. Формирую отчёт.... | ответ: стоп интервью... | оценка: 0.0\n",
            "\n",
            "## Оценки по ходам\n",
            "1:0.30 2:0.00 3:0.00 4:0.20 5:0.00\n",
            "\n",
            "## Затронутые темы\n",
            "проекты распределённые системы high-load архитектура отказоустойчивость архитектурные паттерны\n",
            "\n",
            "## Roadmap\n",
            "- Рекомендуется оценить пробелы по логу интервью и сформировать план развития вручную.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'interview_log.json'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state = run_interview_loop(max_turns=15)\n",
        "save_interview_log(state)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## test_logger() — для финального тестирования\n",
        "\n",
        "<small>прогон сценария по списку ответов пользователя и сохранение лога в interview_log_{номер_сценария}.json.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Сохранено: interview_log_1.json\n"
          ]
        }
      ],
      "source": [
        "def test_logger(scenario_number: int, user_replies: list[str]) -> str:\n",
        "    state = start_interview()\n",
        "    for reply in user_replies:\n",
        "        state = step_interview(state, reply)\n",
        "        if state.get(\"final_feedback\"):\n",
        "            break\n",
        "    filepath = f\"interview_log_{scenario_number}.json\"\n",
        "    save_interview_log(state, filepath, strict_format=True)\n",
        "    return filepath\n",
        "\n",
        "filepath = test_logger(\n",
        "    scenario_number=1,\n",
        "    user_replies=[\n",
        "        \"Имя: Сергей. Позиция: Backend Developer. Грейд:  Senior Опыт: 20 лет в Enterprise (банки, госсектор).\",\n",
        "        \"List — изменяемый, tuple — неизменяемый. List для динамических коллекций, tuple для ключей и констант.\",\n",
        "        \"Стоп интервью. Давай фидбэк.\",\n",
        "    ],\n",
        ")\n",
        "print(f\"Сохранено: {filepath}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
