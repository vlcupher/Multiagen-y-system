{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Agent Interview Coach\n",
        "\n",
        "Мультиагентная система для проведения технического интервью: агенты Interviewer, Observer и Evaluator, скрытая рефлексия, диалог о проектах и технические вопросы, финальный структурированный фидбэк"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.2.7)\n",
            "Requirement already satisfied: langchain-openai in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.1.7)\n",
            "Requirement already satisfied: langchain-anthropic in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.3.1)\n",
            "Requirement already satisfied: langchain-mistralai in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.1.1)\n",
            "Requirement already satisfied: langchain-google-genai in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.2.0)\n",
            "Requirement already satisfied: langgraph in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.0.7)\n",
            "Requirement already satisfied: python-dotenv in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.1.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.7 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain) (1.2.7)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain) (2.12.4)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langgraph) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langgraph) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langgraph) (0.3.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (0.6.6)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.7->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph) (1.12.2)\n",
            "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.6)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (0.24.0)\n",
            "Requirement already satisfied: anyio in c:\\users\\asus\\anaconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (4.10.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\asus\\anaconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\asus\\anaconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\users\\asus\\anaconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-openai) (2.16.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in c:\\users\\asus\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.9.1)\n",
            "Requirement already satisfied: anthropic<1.0.0,>=0.75.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-anthropic) (0.77.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from anthropic<1.0.0,>=0.75.0->langchain-anthropic) (0.17.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.3.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-mistralai) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.15.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-mistralai) (0.22.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tokenizers<1.0.0,>=0.15.1->langchain-mistralai) (1.3.5)\n",
            "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1.0.0,>=0.15.1->langchain-mistralai) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1.0.0,>=0.15.1->langchain-mistralai) (2025.10.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1.0.0,>=0.15.1->langchain-mistralai) (1.2.0)\n",
            "Requirement already satisfied: shellingham in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1.0.0,>=0.15.1->langchain-mistralai) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1.0.0,>=0.15.1->langchain-mistralai) (0.21.1)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.56.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langchain-google-genai) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.48.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (15.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: cryptography>=38.0.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (46.0.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (2.5.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from cryptography>=38.0.3->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.0.0)\n",
            "Requirement already satisfied: pycparser in c:\\users\\asus\\anaconda3\\lib\\site-packages (from cffi>=2.0.0->cryptography>=38.0.3->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.23)\n",
            "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm>4->openai<3.0.0,>=1.109.1->langchain-openai) (0.4.6)\n",
            "Requirement already satisfied: click>=8.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers<1.0.0,>=0.15.1->langchain-mistralai) (8.2.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install langchain langchain-openai langchain-anthropic langchain-mistralai langchain-google-genai langgraph python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from typing import TypedDict, Annotated, Sequence\n",
        "from datetime import datetime\n",
        "import re\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Выбор модели\n",
        "\n",
        "<small>загрузка .env, выбор провайдера LLM и инициализация модели.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM: deepseek\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "LLM_PROVIDER = os.getenv(\"LLM_PROVIDER\", \"deepseek\")\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
        "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\", \"\")\n",
        "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\", \"\")\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"\")\n",
        "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\", \"\")\n",
        "\n",
        "def get_llm():\n",
        "    if LLM_PROVIDER == \"anthropic\":\n",
        "        from langchain_anthropic import ChatAnthropic\n",
        "        return ChatAnthropic(model=\"claude-3-5-sonnet-20241022\", api_key=ANTHROPIC_API_KEY, temperature=0.5)\n",
        "    \n",
        "    elif LLM_PROVIDER == \"gigachat\":\n",
        "        from langchain_openai import ChatOpenAI\n",
        "        return ChatOpenAI(model=\"GigaChat\", base_url=os.getenv(\"GIGACHAT_BASE_URL\", \"https://gigachat.dev/api/v1\"), api_key=os.getenv(\"GIGACHAT_API_KEY\", OPENAI_API_KEY), temperature=0.5)\n",
        "    \n",
        "    elif LLM_PROVIDER == \"mistral\":\n",
        "        from langchain_mistralai import ChatMistralAI\n",
        "        model = os.getenv(\"MISTRAL_MODEL\", \"mistral-small-latest\")\n",
        "        return ChatMistralAI(model=model, api_key=MISTRAL_API_KEY, temperature=0.5)\n",
        "    \n",
        "    elif LLM_PROVIDER == \"gemini\":\n",
        "        from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "        model = os.getenv(\"GOOGLE_MODEL\", \"gemini-1.5-flash\")\n",
        "        return ChatGoogleGenerativeAI(model=model, api_key=GOOGLE_API_KEY, temperature=0.5)\n",
        "    \n",
        "    elif LLM_PROVIDER == \"deepseek\":\n",
        "        from langchain_openai import ChatOpenAI\n",
        "        model = os.getenv(\"DEEPSEEK_MODEL\", \"deepseek-chat\")\n",
        "        return ChatOpenAI(model=model, base_url=\"https://api.deepseek.com\", api_key='API', temperature=0.5)\n",
        "    \n",
        "    else:\n",
        "        from langchain_openai import ChatOpenAI\n",
        "        return ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.5)\n",
        "\n",
        "llm = get_llm()\n",
        "print(f\"LLM: {LLM_PROVIDER}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Устойчивость к вбросам \n",
        "\n",
        "<small>класс проверки off-topic по ключевым словам и редирект-сообщение для возврата к интервью.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RobustnessValidator:\n",
        "    OFF_TOPIC_KEYWORDS = [\n",
        "        \"погода\", \"weather\", \"политика\", \"politics\", \"спорт\", \"sport\",\n",
        "        \"футбол\", \"football\", \"кино\", \"movie\", \"музыка\", \"music\",\n",
        "        \"еда\", \"food\", \"путешествие\", \"travel\"\n",
        "    ]\n",
        "    ON_TOPIC_KEYWORDS = [\n",
        "        \"код\", \"code\", \"программ\", \"program\", \"разработка\", \"develop\",\n",
        "        \"язык\", \"language\", \"библиотека\", \"library\", \"фреймворк\", \"framework\",\n",
        "        \"база данных\", \"database\", \"алгоритм\", \"algorithm\", \"тест\", \"test\",\n",
        "        \"архитектура\", \"architecture\", \"проект\", \"project\", \"опыт\", \"experience\",\n",
        "        \"python\", \"java\", \"sql\", \"api\", \"git\"\n",
        "    ]\n",
        "    EVASION_PATTERNS = [\n",
        "        r\"давай(те)?\\s+поговорим\\s+о\", r\"а\\s+можно\\s+о\", r\"лучше\\s+расскажи\", r\"смени\\s+тему\"\n",
        "    ]\n",
        "\n",
        "    @staticmethod\n",
        "    def is_off_topic(message: str) -> bool:\n",
        "        if not message or len(message.strip().split()) < 3:\n",
        "            return False\n",
        "        low = message.lower()\n",
        "        off = sum(1 for k in RobustnessValidator.OFF_TOPIC_KEYWORDS if k in low)\n",
        "        on = sum(1 for k in RobustnessValidator.ON_TOPIC_KEYWORDS if k in low)\n",
        "        if off > 0 and on == 0:\n",
        "            return True\n",
        "        for p in RobustnessValidator.EVASION_PATTERNS:\n",
        "            if re.search(p, low):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def get_redirect_message() -> str:\n",
        "        return (\n",
        "            \"Спасибо за ваш ответ. Давайте вернёмся к техническим вопросам интервью — \"\n",
        "            \"так я смогу лучше оценить ваши навыки для позиции.\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Состояние графа \n",
        "\n",
        "<small>определение InterviewState (messages, turns, internal_thoughts, difficulty, topics_covered и др.) для графа LangGraph.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "class InterviewState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], \"история сообщений\"]\n",
        "    participant_name: str\n",
        "    position: str\n",
        "    grade: str\n",
        "    experience: str\n",
        "    candidate_name: str\n",
        "    internal_thoughts: Annotated[list[str], \"логи внутренних мыслей агентов\"]\n",
        "    turns: Annotated[list[dict], \"зафиксированные ходы для лога\"]\n",
        "    current_difficulty: str  \n",
        "    topics_covered: Annotated[list[str], \"темы, уже затронутые\"]\n",
        "    stop_requested: bool\n",
        "    final_feedback: str\n",
        "    off_topic_count: int \n",
        "    performance_history: Annotated[list[float], \"оценки Evaluator по ходам 0..1\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Промпты агентов\n",
        "\n",
        "<small>системные промпты для Observer, Interviewer и Evaluator (формат THOUGHT/INSTRUCTION/DIFFICULTY/TOPICS и оценка 0–1).</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "OBSERVER_SYSTEM = \"\"\"Ты — Observer (Наблюдатель) в техническом интервью. Ты не общаешься с кандидатом напрямую.\n",
        "Твоя задача:\n",
        "1. Проанализировать последний ответ кандидата: точность фактов, уверенность, полнота.\n",
        "2. Определить, не ушёл ли разговор off-topic (погода, личное и т.д.) или кандидат не выдал ли ложные факты (галлюцинации). В таких случаях предложи вежливо вернуть беседу в русло интервью.\n",
        "3. Учитывая контекст (позиция: {position}, грейд: {grade}, опыт: {experience}) и уже затронутые темы {topics_covered}, решить:\n",
        "   - какую тему задать дальше (не повторять вопросы, на которые уже был ответ);\n",
        "   - повысить сложность (current_difficulty -> hard), оставить (medium) или понизить (easy), если кандидат \"плывёт\".\n",
        "4. Фаза интервью: как только известны грейд и опыт кандидата (из первого сообщения) — сразу переходи к техническим вопросам по позиции (Python, БД, API и т.д.). Фазы «вопросы о проектах» нет: после представления кандидата — только технические вопросы. Всего около 8 сообщений от кандидата за интервью, за это время нужно проверить его.\n",
        "5. Если кандидат ушёл от ответа, задав встречный вопрос (например, «а как у вас?», «а что вы имеете в виду?»): в INSTRUCTION укажи, что интервьюер должен кратко ответить на вопрос кандидата (1–2 предложения), а затем вежливо вернуть разговор к исходному вопросу и повторить или переформулировать его.\n",
        "6. Выдать ответ СТРОГО в формате (все блоки обязательны):\n",
        "   THOUGHT: [твои выводы и рекомендации на 1-3 предложения]\n",
        "   INSTRUCTION: [что сказать/спросить дальше: конкретный вопрос или действие]\n",
        "   DIFFICULTY: [easy | medium | hard] — следующая сложность вопросов\n",
        "   TOPICS: [тема текущего/следующего вопроса, одна или через запятую]\n",
        "Отвечай только на русском. Не придумывай факты.\"\"\"\n",
        "\n",
        "INTERVIEWER_SYSTEM = \"\"\"Ты — Interviewer (Интервьюер) на техническом интервью. Ты ведёшь диалог с кандидатом.\n",
        "Контекст: позиция {position}, грейд {grade}, опыт кандидата: {experience}. Имя кандидата: {candidate_name}.\n",
        "Ты получил от Observer инструкцию (INSTRUCTION). Выполни её: задай один вопрос или сделай короткий комментарий, затем задай следующий вопрос.\n",
        "Правила:\n",
        "- После того как узнали грейд и опыт кандидата (из представления) — сразу переходи к техническим вопросам по позиции. Отдельной фазы про проекты нет.\n",
        "- Говори от первого лица (интервьюер). Один блок реплики — без разметки, без \"Interviewer:\".\n",
        "- Если инструкция говорит вернуть беседу в русло — вежливо переведи разговор обратно к теме интервью.\n",
        "- Не задавай вопросов, на которые кандидат уже ответил в этой беседе.\n",
        "- Адаптируй тон: при слабом ответе можно дать подсказку или упростить следующий вопрос.\n",
        "- Если кандидат вместо ответа задал встречный вопрос (уклонился от ответа): сначала кратко (1–2 предложения) ответь на его вопрос, затем вежливо верни разговор к своему исходному вопросу и повтори или переформулируй его, чтобы получить ответ.\n",
        "Отвечай только текстом реплики для кандидата, без префиксов.\"\"\"\n",
        "\n",
        "EVALUATOR_SYSTEM = \"\"\"Ты — Evaluator (Оценщик) в техническом интервью. Ты не общаешься с кандидатом.\n",
        "Оцени ответ кандидата на заданный вопрос: фактическая корректность (correct/partial/incorrect), полнота, глубина.\n",
        "Дай числовую оценку 0.0–1.0. Если ответ с ошибками — укажи правильный ответ в блоке ПРАВИЛЬНЫЙ_ОТВЕТ:.\n",
        "Формат ответа:\n",
        "ОЦЕНКА: число 0.0–1.0\n",
        "КОРРЕКТНОСТЬ: correct | partial | incorrect\n",
        "КОММЕНТАРИЙ: 1–2 предложения\n",
        "ПРАВИЛЬНЫЙ_ОТВЕТ: (если нужен — кратко)\n",
        "Отвечай на русском.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Скрытая рефлексия\n",
        "\n",
        "<small>узел Observer: анализ последнего ответа кандидата, вызов LLM, парсинг DIFFICULTY/TOPICS и обновление internal_thoughts.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _format_history(messages: Sequence[BaseMessage], max_last: int = 12) -> str:\n",
        "    parts = []\n",
        "    for m in list(messages)[-max_last:]:\n",
        "        if isinstance(m, HumanMessage):\n",
        "            parts.append(f\"Кандидат: {m.content}\")\n",
        "        else:\n",
        "            parts.append(f\"Интервьюер: {m.content}\")\n",
        "    return \"\\n\".join(parts) if parts else \"(пока нет реплик)\"\n",
        "\n",
        "def observer_node(state: InterviewState) -> dict:\n",
        "    messages = state[\"messages\"]\n",
        "    last_user = None\n",
        "    for m in reversed(messages):\n",
        "        if isinstance(m, HumanMessage):\n",
        "            last_user = m.content\n",
        "            break\n",
        "    if not last_user:\n",
        "        last_user = \"(приветствие, ожидаем первый ответ)\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", OBSERVER_SYSTEM),\n",
        "        (\"human\", \"История диалога:\\n{history}\\n\\nПоследний ответ кандидата: {last_reply}\\n\\nГрейд и опыт уже известны: {intro_known}. Если да — задавай только технический вопрос по позиции; иначе можно уточнить представление. Текущая сложность: {difficulty}. Дай THOUGHT, INSTRUCTION, DIFFICULTY и TOPICS.\")\n",
        "    ])\n",
        "    chain = prompt | llm\n",
        "    intro_known = bool((state.get(\"grade\") or \"\").strip() and (state.get(\"experience\") or \"\").strip())\n",
        "    response = chain.invoke({\n",
        "        \"position\": state.get(\"position\", \"\") or \"(не указано)\",\n",
        "        \"grade\": state.get(\"grade\", \"\") or \"(не указано)\",\n",
        "        \"experience\": state.get(\"experience\", \"\") or \"(не указано)\",\n",
        "        \"topics_covered\": state.get(\"topics_covered\") or [],\n",
        "        \"history\": _format_history(messages),\n",
        "        \"last_reply\": last_user,\n",
        "        \"intro_known\": \"да\" if intro_known else \"нет\",\n",
        "        \"difficulty\": state.get(\"current_difficulty\", \"medium\"),\n",
        "    })\n",
        "    text = response.content if hasattr(response, \"content\") else str(response)\n",
        "    thoughts = state.get(\"internal_thoughts\") or []\n",
        "    thoughts.append(f\"[Observer]: {text}\")\n",
        "    out = {\"internal_thoughts\": thoughts}\n",
        "    if \"DIFFICULTY:\" in text:\n",
        "        d = text.split(\"DIFFICULTY:\")[-1].strip().split()[0].lower()\n",
        "        if d in (\"easy\", \"medium\", \"hard\"):\n",
        "            out[\"current_difficulty\"] = d\n",
        "    if \"TOPICS:\" in text:\n",
        "        raw = text.split(\"TOPICS:\")[-1].strip().split(\"THOUGHT:\")[0].strip().split(\"INSTRUCTION:\")[0].strip()\n",
        "        new_topics = [t.strip() for t in raw.split(\",\") if t.strip()]\n",
        "        if new_topics:\n",
        "            prev = list(state.get(\"topics_covered\") or [])\n",
        "            out[\"topics_covered\"] = prev + [t for t in new_topics if t not in prev]\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluator (скрытый оценщик)\n",
        "\n",
        "<small>узел Evaluator: оценка ответа кандидата 0–1 по вопросу интервьюера, дополнение performance_history и internal_thoughts.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluator_node(state: InterviewState) -> dict:\n",
        "    messages = state.get(\"messages\") or []\n",
        "    last_user, last_question = \"\", \"\"\n",
        "    for m in reversed(messages):\n",
        "        if isinstance(m, HumanMessage):\n",
        "            last_user = m.content or \"\"\n",
        "            break\n",
        "    for m in reversed(messages):\n",
        "        if isinstance(m, AIMessage):\n",
        "            last_question = m.content or \"\"\n",
        "            break\n",
        "    if not last_user:\n",
        "        return {\"performance_history\": list(state.get(\"performance_history\") or []), \"internal_thoughts\": state.get(\"internal_thoughts\") or []}\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", EVALUATOR_SYSTEM),\n",
        "        (\"human\", \"Вопрос интервьюера: {question}\\n\\nОтвет кандидата: {reply}\\n\\nДай ОЦЕНКА, КОРРЕКТНОСТЬ, КОММЕНТАРИЙ и при необходимости ПРАВИЛЬНЫЙ_ОТВЕТ.\")\n",
        "    ])\n",
        "    chain = prompt | llm\n",
        "    resp = chain.invoke({\"question\": last_question, \"reply\": last_user})\n",
        "    text = resp.content if hasattr(resp, \"content\") else str(resp)\n",
        "    score = 0.5\n",
        "    m = re.search(r'ОЦЕНКА:\\s*([0-9.]+)', text, re.I)\n",
        "    if m:\n",
        "        try:\n",
        "            score = min(1.0, max(0.0, float(m.group(1))))\n",
        "        except ValueError:\n",
        "            pass\n",
        "    hist = list(state.get(\"performance_history\") or [])\n",
        "    hist.append(score)\n",
        "    thoughts = list(state.get(\"internal_thoughts\") or [])\n",
        "    thoughts.append(f\"[Evaluator]: {text[:300]}...\" if len(text) > 300 else f\"[Evaluator]: {text}\")\n",
        "    return {\"performance_history\": hist, \"internal_thoughts\": thoughts}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Interviewer (видимая реплика)\n",
        "\n",
        "<small>узел Interviewer: извлечение INSTRUCTION из Observer, вызов LLM и добавление одной реплики в messages.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _extract_instruction(observer_text: str) -> str:\n",
        "    if \"INSTRUCTION:\" in observer_text:\n",
        "        block = observer_text.split(\"INSTRUCTION:\")[-1].strip()\n",
        "        if \"DIFFICULTY:\" in block:\n",
        "            block = block.split(\"DIFFICULTY:\")[0].strip()\n",
        "        if \"THOUGHT:\" in block:\n",
        "            block = block.split(\"THOUGHT:\")[0].strip()\n",
        "        return block\n",
        "    return observer_text\n",
        "\n",
        "def interviewer_node(state: InterviewState) -> dict:\n",
        "    thoughts = state.get(\"internal_thoughts\") or []\n",
        "    last_thought = thoughts[-1] if thoughts else \"\"\n",
        "    instruction = _extract_instruction(last_thought)\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", INTERVIEWER_SYSTEM),\n",
        "        (\"human\", \"Инструкция от Observer: {instruction}\\n\\nИстория диалога (последние реплики):\\n{history}\\n\\nНапиши только одну реплику интервьюера для кандидата.\")\n",
        "    ])\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({\n",
        "        \"position\": state[\"position\"],\n",
        "        \"grade\": state[\"grade\"],\n",
        "        \"experience\": state[\"experience\"],\n",
        "        \"participant_name\": state.get(\"participant_name\", \"\"),\n",
        "        \"candidate_name\": state.get(\"candidate_name\") or \"Кандидат\",\n",
        "        \"instruction\": instruction,\n",
        "        \"history\": _format_history(state[\"messages\"]),\n",
        "    })\n",
        "    reply = response.content if hasattr(response, \"content\") else str(response)\n",
        "    reply = reply.strip().strip('\"')\n",
        "    new_messages = list(state[\"messages\"]) + [AIMessage(content=reply)]\n",
        "    return {\"messages\": new_messages}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Условие: стоп или продолжение\n",
        "\n",
        "<small>STOP_PHRASES, should_stop, should_redirect, off_topic_check_node и redirect_save_node для ветвления графа.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function __main__.redirect_save_node(state: __main__.InterviewState) -> dict>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "STOP_PHRASES = [\"стоп интервью\", \"завершить интервью\", \"закончить\", \"стоп\", \"конец интервью\", \"finish\"]\n",
        "\n",
        "def should_stop(state: InterviewState) -> str:\n",
        "    if state.get(\"stop_requested\"):\n",
        "        return \"finish\"\n",
        "    messages = state.get(\"messages\") or []\n",
        "    for m in reversed(messages):\n",
        "        if isinstance(m, HumanMessage):\n",
        "            lower = (m.content or \"\").strip().lower()\n",
        "            if any(p in lower for p in STOP_PHRASES):\n",
        "                return \"finish\"\n",
        "            break\n",
        "    return \"continue\"\n",
        "\n",
        "def should_redirect(state: InterviewState) -> str:\n",
        "    if state.get(\"off_topic_count\", 0) >= 2:\n",
        "        return \"redirect\"\n",
        "    return \"continue\"\n",
        "\n",
        "def off_topic_check_node(state: InterviewState) -> dict:\n",
        "    last_user = \"\"\n",
        "    for m in reversed(state.get(\"messages\") or []):\n",
        "        if isinstance(m, HumanMessage):\n",
        "            last_user = (m.content or \"\").strip()\n",
        "            break\n",
        "    count = state.get(\"off_topic_count\", 0)\n",
        "    if last_user and RobustnessValidator.is_off_topic(last_user):\n",
        "        count += 1\n",
        "    return {\"off_topic_count\": count}\n",
        "\n",
        "def redirect_save_node(state: InterviewState) -> dict:\n",
        "    msg = RobustnessValidator.get_redirect_message()\n",
        "    turns = list(state.get(\"turns\") or [])\n",
        "    thoughts = list(state.get(\"internal_thoughts\") or [])\n",
        "    thoughts.append(\"[Observer]: Off-topic. Redirecting.\")\n",
        "    user_msg = \"\"\n",
        "    for m in reversed(state.get(\"messages\") or []):\n",
        "        if isinstance(m, HumanMessage):\n",
        "            user_msg = m.content or \"\"\n",
        "            break\n",
        "    turns.append({\n",
        "        \"turn_id\": len(turns) + 1,\n",
        "        \"agent_visible_message\": msg,\n",
        "        \"user_message\": user_msg,\n",
        "        \"internal_thoughts\": \"[Observer]: Off-topic.\\n[Interviewer]: Redirecting.\\n\",\n",
        "        \"performance_metrics\": {\"score\": 0.0},\n",
        "    })\n",
        "    new_messages = list(state.get(\"messages\") or []) + [AIMessage(content=msg)]\n",
        "    return {\"messages\": new_messages, \"internal_thoughts\": thoughts, \"turns\": turns}\n",
        "\n",
        "redirect_save_node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Регистрация хода в лог (turn)\n",
        "\n",
        "<small>save_turn_node и log_stop_turn_node: форматирование internal_thoughts для лога и добавление хода в turns.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _extract_thought(observer_text: str) -> str:\n",
        "    if \"THOUGHT:\" in observer_text:\n",
        "        block = observer_text.split(\"THOUGHT:\")[1].strip()\n",
        "        if \"INSTRUCTION:\" in block:\n",
        "            block = block.split(\"INSTRUCTION:\")[0].strip()\n",
        "        return block[:500] if len(block) > 500 else block\n",
        "    return observer_text[:500] if observer_text else \"\"\n",
        "\n",
        "def _extract_evaluator_thought(text: str) -> str:\n",
        "    if \"[Evaluator]:\" in text:\n",
        "        return text.split(\"[Evaluator]:\")[-1].strip()[:400]\n",
        "    return text[:400] if text else \"\"\n",
        "\n",
        "def _build_internal_thoughts_for_log(thoughts: list, agent_msg: str) -> str:\n",
        "    \"\"\"Формат: каждое сообщение [agent_name]: thought\\n (по спецификации).\"\"\"\n",
        "    parts = []\n",
        "    if len(thoughts) >= 2:\n",
        "        parts.append(\"[Observer]: \" + _extract_thought(thoughts[-2]) + \"\\n\")\n",
        "    elif thoughts:\n",
        "        parts.append(\"[Observer]: \" + _extract_thought(thoughts[-1]) + \"\\n\")\n",
        "    if thoughts and \"[Evaluator]\" in (thoughts[-1] or \"\"):\n",
        "        parts.append(\"[Evaluator]: \" + _extract_evaluator_thought(thoughts[-1]) + \"\\n\")\n",
        "    interviewer_part = (agent_msg.split(\".\")[0].strip() + \".\") if agent_msg else \"Задал следующий вопрос.\"\n",
        "    parts.append(\"[Interviewer]: \" + interviewer_part + \"\\n\")\n",
        "    return \"\".join(parts)\n",
        "\n",
        "def save_turn_node(state: InterviewState) -> dict:\n",
        "    messages = list(state[\"messages\"])\n",
        "    turns = list(state.get(\"turns\") or [])\n",
        "    thoughts = state.get(\"internal_thoughts\") or []\n",
        "    user_msg = \"\"\n",
        "    agent_msg = \"\"\n",
        "    last_ai_before_user = \"\"\n",
        "    for m in messages:\n",
        "        if isinstance(m, HumanMessage):\n",
        "            user_msg = m.content or \"\"\n",
        "            agent_msg = last_ai_before_user\n",
        "        else:\n",
        "            last_ai_before_user = m.content or \"\"\n",
        "    if not agent_msg:\n",
        "        agent_msg = last_ai_before_user\n",
        "    new_agent_msg = (messages[-1].content if messages and isinstance(messages[-1], AIMessage) else \"\") or last_ai_before_user\n",
        "    turn_id = len(turns) + 1\n",
        "    internal_str = _build_internal_thoughts_for_log(thoughts, new_agent_msg)\n",
        "    perf = state.get(\"performance_history\") or []\n",
        "    score = perf[-1] if perf else 0.5\n",
        "    turns.append({\n",
        "        \"turn_id\": turn_id,\n",
        "        \"agent_visible_message\": agent_msg,\n",
        "        \"user_message\": user_msg,\n",
        "        \"internal_thoughts\": internal_str,\n",
        "        \"performance_metrics\": {\"score\": score},\n",
        "    })\n",
        "    return {\"turns\": turns}\n",
        "\n",
        "def log_stop_turn_node(state: InterviewState) -> dict:\n",
        "    turns = list(state.get(\"turns\") or [])\n",
        "    last_user = \"\"\n",
        "    for m in reversed(state.get(\"messages\") or []):\n",
        "        if isinstance(m, HumanMessage):\n",
        "            last_user = (m.content or \"\").strip()\n",
        "            break\n",
        "    if not last_user:\n",
        "        return {}\n",
        "    last_lower = last_user.lower()\n",
        "    is_stop = any(p in last_lower for p in STOP_PHRASES)\n",
        "    already_logged = turns and (turns[-1].get(\"user_message\") or \"\").strip() == last_user\n",
        "    if is_stop and not already_logged:\n",
        "        turn_id = len(turns) + 1\n",
        "        turns.append({\n",
        "            \"turn_id\": turn_id,\n",
        "            \"agent_visible_message\": \"Интервью завершено по запросу. Формирую отчёт.\",\n",
        "            \"user_message\": last_user,\n",
        "            \"internal_thoughts\": \"[Observer]: Запрос на завершение.\\n[Interviewer]: Завершение интервью.\\n\",\n",
        "            \"performance_metrics\": {\"score\": 0.0},\n",
        "        })\n",
        "        return {\"turns\": turns}\n",
        "    return {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Генерация финального фидбэка\n",
        "\n",
        "<small>feedback_node: вызов LLM с FEEDBACK_SYSTEM, парсинг JSON из ответа и формирование структурированного отчёта.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "FEEDBACK_SYSTEM = \"\"\"Ты — эксперт по подведению итогов технического интервью. По истории диалога сформируй структурированный отчёт.\n",
        "Кандидат (имя из представления): {candidate_name}. Позиция: {position}, грейд: {grade}, опыт: {experience}.\n",
        "Сформируй отчёт строго в следующей структуре (на русском):\n",
        "\n",
        "## Вердикт (Decision)\n",
        "- **Grade:** Junior | Middle | Senior (выбери один)\n",
        "- **Hiring Recommendation:** Hire | No Hire | Strong Hire\n",
        "- **Confidence Score:** число 0-100 (насколько уверен в оценке)\n",
        "\n",
        "## Анализ Hard Skills (Technical Review)\n",
        "- **Confirmed Skills:** темы, где кандидат дал точные ответы (список).\n",
        "- **Knowledge Gaps:** темы с ошибками или \"не знаю\"; для каждой приведи кратко правильный ответ.\n",
        "\n",
        "## Soft Skills & Communication\n",
        "- **Clarity:** насколько понятно излагал мысли.\n",
        "- **Honesty:** честность (признание незнания vs попытки выкрутиться).\n",
        "- **Engagement:** вовлечённость, встречные вопросы.\n",
        "\n",
        "## Персональный Roadmap (Next Steps)\n",
        "- Конкретные темы/технологии для подтягивания (список).\n",
        "- Опционально: ссылки на документацию или статьи по темам.\n",
        "\n",
        "Используй только факты из диалога. Не выдумывай.\n",
        "При завершении интервью по запросу кандидата фидбэк должен быть осознанным: кратко опиши ответы кандидата по каждому вопросу (что спросили, что ответил, насколько полно и корректно), сделай выводы по ним и сформулируй вердикт на основе этих описаний.\n",
        "Опционально в конце можно добавить JSON: {\"grade\": \"...\", \"hiring_recommendation\": \"...\", \"confidence_score\": N, \"knowledge_gaps\": [{\"topic\": \"...\", \"user_answer\": \"...\", \"correct_answer\": \"...\"}], \"confirmed_skills\": [], \"roadmap\": []}.\"\"\"\n",
        "\n",
        "def _parse_feedback_json(response_text: str) -> dict | None:\n",
        "    try:\n",
        "        start = response_text.find(\"{\")\n",
        "        end = response_text.rfind(\"}\") + 1\n",
        "        if start != -1 and end > start:\n",
        "            return json.loads(response_text[start:end])\n",
        "    except json.JSONDecodeError:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "def _feedback_from_parsed(data: dict) -> str:\n",
        "    parts = []\n",
        "    parts.append(\"## Вердикт (Decision)\")\n",
        "    parts.append(f\"- **Grade:** {data.get('grade', 'N/A')}\")\n",
        "    parts.append(f\"- **Hiring Recommendation:** {data.get('hiring_recommendation', 'N/A')}\")\n",
        "    parts.append(f\"- **Confidence Score:** {data.get('confidence_score', 0)}\")\n",
        "    parts.append(\"\")\n",
        "    parts.append(\"## Анализ Hard Skills\")\n",
        "    parts.append(\"- **Confirmed Skills:** \" + \", \".join(data.get(\"confirmed_skills\", [])))\n",
        "    gaps = data.get(\"knowledge_gaps\", [])\n",
        "    if gaps:\n",
        "        parts.append(\"- **Knowledge Gaps:**\")\n",
        "        for g in gaps:\n",
        "            if isinstance(g, dict):\n",
        "                parts.append(f\"  - {g.get('topic', '')}: правильный ответ — {g.get('correct_answer', '')[:200]}\")\n",
        "    parts.append(\"\")\n",
        "    parts.append(\"## Roadmap\")\n",
        "    parts.append(\"\\n\".join(\"- \" + x for x in data.get(\"roadmap\", [])))\n",
        "    return \"\\n\".join(parts)\n",
        "\n",
        "def _build_fallback_feedback(state: InterviewState) -> str:\n",
        "    \"\"\"Формирует осознанный структурированный фидбэк из состояния без вызова LLM: описание ответов кандидата и выводы.\"\"\"\n",
        "    parts = []\n",
        "    parts.append(\"## Контекст кандидата\")\n",
        "    parts.append(f\"- **Кандидат:** {state.get('candidate_name') or '—'}\")\n",
        "    parts.append(f\"- **Позиция:** {state.get('position') or '—'}; **грейд:** {state.get('grade') or '—'}\")\n",
        "    parts.append(f\"- **Опыт:** {(state.get('experience') or '—')[:400]}\")\n",
        "    turns = state.get(\"turns\") or []\n",
        "    perf = state.get(\"performance_history\") or []\n",
        "    parts.append(\"\")\n",
        "    parts.append(\"## Описание ответов кандидата по ходам интервью\")\n",
        "    for i, t in enumerate(turns):\n",
        "        tid = t.get(\"turn_id\", i + 1)\n",
        "        q = (t.get(\"agent_visible_message\") or \"—\").strip()\n",
        "        a = (t.get(\"user_message\") or \"—\").strip()\n",
        "        sc = t.get(\"performance_metrics\", {}).get(\"score\")\n",
        "        sc_str = f\"{sc:.2f}\" if isinstance(sc, (int, float)) else \"—\"\n",
        "        parts.append(f\"### Ход {tid}\")\n",
        "        parts.append(f\"- **Вопрос:** {q}\")\n",
        "        parts.append(f\"- **Ответ кандидата:** {a}\")\n",
        "        parts.append(f\"- **Оценка ответа (0–1):** {sc_str}\")\n",
        "        parts.append(\"\")\n",
        "    parts.append(\"## Краткие выводы по ответам\")\n",
        "    if perf:\n",
        "        avg = sum(perf) / len(perf)\n",
        "        strong = sum(1 for p in perf if p >= 0.7)\n",
        "        weak = sum(1 for p in perf if p < 0.5)\n",
        "        parts.append(f\"- Всего ходов с оценкой: {len(perf)}; средняя оценка: {avg:.2f}.\")\n",
        "        parts.append(f\"- Ответов с оценкой ≥ 0.7: {strong}; с оценкой < 0.5: {weak}.\")\n",
        "    topics = state.get(\"topics_covered\") or []\n",
        "    if topics:\n",
        "        parts.append(f\"- Затронутые темы: {', '.join(topics)}.\")\n",
        "    parts.append(\"\")\n",
        "    parts.append(\"## Вердикт (Decision)\")\n",
        "    parts.append(\"- **Grade:** по данным диалога — см. описание ответов выше\")\n",
        "    parts.append(\"- **Hiring Recommendation:** требуется ручная оценка по описанию ответов кандидата\")\n",
        "    parts.append(\"- **Confidence Score:** 0 (отчёт сформирован по данным без LLM)\")\n",
        "    parts.append(\"\")\n",
        "    parts.append(\"## Roadmap\")\n",
        "    parts.append(\"- Рекомендуется оценить пробелы по описанию ответов выше и сформировать план развития.\")\n",
        "    return \"\\n\".join(parts)\n",
        "\n",
        "def feedback_node(state: InterviewState) -> dict:\n",
        "    messages = state.get(\"messages\") or []\n",
        "    history = _format_history(messages, max_last=50) if messages else \"(диалог пуст)\"\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", FEEDBACK_SYSTEM),\n",
        "        (\"human\", \"История диалога:\\n{history}\")\n",
        "    ])\n",
        "    chain = prompt | llm\n",
        "    try:\n",
        "        response = chain.invoke({\n",
        "            \"candidate_name\": state.get(\"candidate_name\") or \"Кандидат\",\n",
        "            \"position\": state.get(\"position\", \"\") or \"(не указано)\",\n",
        "            \"grade\": state.get(\"grade\", \"\") or \"(не указано)\",\n",
        "            \"experience\": state.get(\"experience\", \"\") or \"(не указано)\",\n",
        "            \"history\": history,\n",
        "        })\n",
        "        text = response.content if hasattr(response, \"content\") else str(response)\n",
        "        parsed = _parse_feedback_json(text)\n",
        "        if parsed:\n",
        "            text = _feedback_from_parsed(parsed)\n",
        "    except Exception:\n",
        "        text = _build_fallback_feedback(state)\n",
        "    return {\"final_feedback\": text}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Сборка графа LangGraph\n",
        "\n",
        "<small>build_graph: создание StateGraph с узлами и условными рёбрами (router → off_topic_check → observer → evaluator → interviewer → save_turn и т.д.).</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph built.\n"
          ]
        }
      ],
      "source": [
        "def router_node(state: InterviewState) -> dict:\n",
        "    return {}\n",
        "\n",
        "def build_graph():\n",
        "    graph = StateGraph(InterviewState)\n",
        "    graph.add_node(\"router\", router_node)\n",
        "    graph.add_node(\"observer\", observer_node)\n",
        "    graph.add_node(\"evaluator\", evaluator_node)\n",
        "    graph.add_node(\"interviewer\", interviewer_node)\n",
        "    graph.add_node(\"save_turn\", save_turn_node)\n",
        "    graph.add_node(\"log_stop_turn\", log_stop_turn_node)\n",
        "    graph.add_node(\"feedback\", feedback_node)\n",
        "\n",
        "    graph.add_node(\"off_topic_check\", off_topic_check_node)\n",
        "    graph.add_node(\"redirect_save\", redirect_save_node)\n",
        "    graph.set_entry_point(\"router\")\n",
        "    graph.add_conditional_edges(\"router\", should_stop, {\"continue\": \"off_topic_check\", \"finish\": \"log_stop_turn\"})\n",
        "    graph.add_conditional_edges(\"off_topic_check\", should_redirect, {\"redirect\": \"redirect_save\", \"continue\": \"observer\"})\n",
        "    graph.add_edge(\"redirect_save\", END)\n",
        "    graph.add_edge(\"observer\", \"evaluator\")\n",
        "    graph.add_edge(\"evaluator\", \"interviewer\")\n",
        "    graph.add_edge(\"interviewer\", \"save_turn\")\n",
        "    graph.add_conditional_edges(\"save_turn\", should_stop, {\"continue\": END, \"finish\": \"log_stop_turn\"})\n",
        "    graph.add_edge(\"log_stop_turn\", \"feedback\")\n",
        "    graph.add_edge(\"feedback\", END)\n",
        "\n",
        "    return graph.compile()\n",
        "\n",
        "interview_graph = build_graph()\n",
        "print(\"Graph built.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Старт интервью (вводные)\n",
        "\n",
        "<small>start_interview() возвращает начальный state с приветствием; parse_intro_from_message извлекает позицию, грейд и опыт из первого сообщения.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "TESTER_NAME = \"Григорьев Владимир Сергеевич\"\n",
        "\n",
        "def start_interview() -> InterviewState:\n",
        "    \"\"\"Инициализация без параметров кандидата: имя, роль, грейд, опыт вводит сам человек в первом сообщении.\"\"\"\n",
        "    initial_msg = (\n",
        "        \"Здравствуйте! Представьтесь, пожалуйста: как вас зовут, на какую позицию и грейд претендуете, кратко об опыте. \"\n",
        "        \"Тогда начнём интервью.\"\n",
        "    )\n",
        "    return {\n",
        "        \"messages\": [AIMessage(content=initial_msg)],\n",
        "        \"participant_name\": TESTER_NAME,\n",
        "        \"position\": \"\",\n",
        "        \"grade\": \"\",\n",
        "        \"experience\": \"\",\n",
        "        \"candidate_name\": \"\",\n",
        "        \"internal_thoughts\": [\"[Observer]: Старт интервью. [Interviewer]: Запрос представления.\"],\n",
        "        \"turns\": [],\n",
        "        \"current_difficulty\": \"medium\",\n",
        "        \"topics_covered\": [],\n",
        "        \"stop_requested\": False,\n",
        "        \"final_feedback\": \"\",\n",
        "        \"off_topic_count\": 0,\n",
        "        \"performance_history\": [],\n",
        "    }\n",
        "\n",
        "def parse_intro_from_message(text: str) -> dict:\n",
        "    \"\"\"Из первого сообщения кандидата извлекаем позицию, грейд, опыт (и имя при возможности).\"\"\"\n",
        "    t = (text or \"\").strip()\n",
        "    out = {\"position\": \"\", \"grade\": \"\", \"experience\": t[:500], \"candidate_name\": \"\"}\n",
        "    low = t.lower()\n",
        "    for g in (\"junior\", \"middle\", \"senior\", \"джуниор\", \"мидл\", \"сеньор\", \"младший\", \"старший\"):\n",
        "        if g in low:\n",
        "            out[\"grade\"] = g\n",
        "            break\n",
        "    for p in (\"python\", \"backend\", \"frontend\", \"разработчик\", \"developer\", \"инженер\", \"engineer\"):\n",
        "        if p in low:\n",
        "            out[\"position\"] = t[:200] if len(t) < 200 else (t[:100] + \"...\")\n",
        "            break\n",
        "    if not out[\"position\"]:\n",
        "        out[\"position\"] = t[:150] if len(t) < 150 else (t[:80] + \"...\")\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Один шаг диалога (вопрос — ответ)\n",
        "\n",
        "<small>step_interview добавляет сообщение пользователя, вызывает граф и при первом ответе заполняет контекст кандидата; get_last_agent_message и get_last_internal_thoughts для вывода.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _is_stop_phrase(msg: str) -> bool:\n",
        "    lower = (msg or \"\").strip().lower()\n",
        "    return any(p in lower for p in [\"стоп интервью\", \"завершить\", \"конец интервью\", \"давай фидбэк\", \"закончить\", \"стоп\", \"finish\"])\n",
        "\n",
        "def _ensure_feedback_on_stop(state: InterviewState) -> InterviewState:\n",
        "    \"\"\"При стопе всегда формируем фидбэк: log_stop_turn + feedback_node.\"\"\"\n",
        "    log_update = log_stop_turn_node(state)\n",
        "    state = {**state, **log_update}\n",
        "    try:\n",
        "        feedback_update = feedback_node(state)\n",
        "        return {**state, **feedback_update}\n",
        "    except Exception:\n",
        "        return {**state, \"final_feedback\": _build_fallback_feedback(state)}\n",
        "\n",
        "def step_interview(current_state: InterviewState, user_message: str) -> InterviewState:\n",
        "    new_messages = list(current_state.get(\"messages\") or []) + [HumanMessage(content=user_message)]\n",
        "    new_state = {**current_state, \"messages\": new_messages}\n",
        "    # Сразу заполняем грейд/опыт из сообщения, чтобы Observer при первом же ответе переходил к техническим вопросам\n",
        "    if user_message.strip():\n",
        "        parsed = parse_intro_from_message(user_message)\n",
        "        for k in (\"position\", \"grade\", \"experience\", \"candidate_name\"):\n",
        "            if parsed.get(k) and not (new_state.get(k) or \"\").strip():\n",
        "                new_state[k] = parsed.get(k) or new_state.get(k) or \"\"\n",
        "    result = None\n",
        "    try:\n",
        "        result = interview_graph.invoke(new_state)\n",
        "    except Exception as e:\n",
        "        if _is_stop_phrase(user_message):\n",
        "            result = _ensure_feedback_on_stop(new_state)\n",
        "        else:\n",
        "            raise\n",
        "    # Гарантия: если пользователь сказал стоп, но фидбэка нет — формируем вручную\n",
        "    if _is_stop_phrase(user_message) and not (result.get(\"final_feedback\") or \"\").strip():\n",
        "        result = _ensure_feedback_on_stop(result if result else new_state)\n",
        "    if not (result.get(\"position\") or result.get(\"experience\")) and user_message.strip():\n",
        "        parsed = parse_intro_from_message(user_message)\n",
        "        result = {**result, \"position\": parsed.get(\"position\") or result.get(\"position\", \"\"), \"grade\": parsed.get(\"grade\") or result.get(\"grade\", \"\"), \"experience\": parsed.get(\"experience\") or result.get(\"experience\", \"\"), \"candidate_name\": parsed.get(\"candidate_name\") or result.get(\"candidate_name\", \"\")}\n",
        "    return result\n",
        "\n",
        "def get_last_agent_message(state: InterviewState) -> str:\n",
        "    for m in reversed(state.get(\"messages\") or []):\n",
        "        if isinstance(m, AIMessage):\n",
        "            return m.content or \"\"\n",
        "    return \"\"\n",
        "\n",
        "def get_last_internal_thoughts(state: InterviewState, last_n: int = 2) -> list:\n",
        "    thoughts = state.get(\"internal_thoughts\") or []\n",
        "    return thoughts[-last_n:] if len(thoughts) >= last_n else thoughts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Цикл интервью \n",
        "\n",
        "<small>run_interview_loop: интерактивный цикл с input, вывод реплики интервьюера и internal_thoughts до появления final_feedback.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_interview_loop(max_turns: int = 15):\n",
        "    state = start_interview()\n",
        "    print(\"--- Приветствие ---\")\n",
        "    print(get_last_agent_message(state))\n",
        "    print()\n",
        "\n",
        "    for _ in range(max_turns - 1):\n",
        "        user_input = input(\"Вы (кандидат): \").strip()\n",
        "        if not user_input:\n",
        "            continue\n",
        "        state = step_interview(state, user_input)\n",
        "        if state.get(\"final_feedback\"):\n",
        "            break\n",
        "        print(\"--- Интервьюер ---\")\n",
        "        print(get_last_agent_message(state))\n",
        "        print(\"--- Внутренние мысли (логи) ---\")\n",
        "        for t in get_last_internal_thoughts(state):\n",
        "            print(t[:300] + \"...\" if len(t) > 300 else t)\n",
        "        print()\n",
        "\n",
        "    if state.get(\"final_feedback\"):\n",
        "        print(\"\\n=== Финальный фидбэк ===\")\n",
        "        print(state[\"final_feedback\"])\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Запуск интервью (пример без input — для автопроверки)\n",
        "\n",
        "<small>автозапуск с тестовыми ответами test_replies без запроса ввода для быстрой проверки пайплайна.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Приветствие: Здравствуйте! Представьтесь, пожалуйста: как вас зовут, на какую позицию и грейд претендуете, кратко об опыте. Тогда начнём интервью. \n",
            "\n",
            "Кандидат: Я джун, пишу код 3 месяца на Python.\n",
            "Интервьюер: Хорошо, спасибо. Тогда сразу перейдём к техническим вопросам по Python. Расскажите, пожалуйста, чем отличаются списки (list) от кортежей (tuple) в Python?\n",
            "Мысли: ['[Observer]: THOUGHT: Кандидат представился кратко: указал грейд (джун) и опыт (3 месяца на Python), но не назвал имя и позицию. Однако для начала интервью этого достаточно — грейд и опыт известны, можно сразу переходить к техническим вопросам по позиции (Python-разработчик). Сложность оставим medium, так как кандидат — джун, но нужно проверить базовые знания.\\n\\nINSTRUCTION: Отлично, спасибо за представление. Тогда начнём с технических вопросов по Python. Расскажите, в чём разница между списком (list) и кортежем (tuple) в Python? Приведите примеры использования каждого.\\n\\nDIFFICULTY: medium  \\nTOPICS: Python, структуры данных', '[Evaluator]: ОЦЕНКА: 0.2\\nКОРРЕКТНОСТЬ: partial\\nКОММЕНТАРИЙ: Кандидат дал лишь фрагментарный ответ, не представившись по имени, не указав позицию и грейд, а также не предоставив структурированной информации об опыте.\\nПРАВИЛЬНЫЙ_ОТВЕТ: Полный ответ должен включать имя, целевую позицию и грейд (например, \"Python-ра...']\n",
            "\n",
            "Кандидат: Знаю списки, словари, функции. Декораторы использовал в FastAPI.\n",
            "Интервьюер: Я понимаю, что вы знакомы с разными концепциями, но давайте вернёмся к конкретному вопросу. Чем отличаются списки (list) от кортежей (tuple) в Python? Например, в изменяемости, синтаксисе или использовании.\n",
            "Мысли: ['[Observer]: THOUGHT: Кандидат не ответил на вопрос про отличия list и tuple, а перечислил знакомые темы. Это уход от ответа, но не off-topic. Нужно вежливо вернуть к вопросу, возможно, он не понял его. Опыт и грейд уже известны из первого сообщения (джун, 3 месяца на Python), поэтому продолжаем технические вопросы. Сложность оставляем medium, так как вопрос базовый, но кандидат не справился с ним напрямую.\\n\\nINSTRUCTION: \"Я понимаю, что вы знакомы со списками, словарями и декораторами. Но давайте вернёмся к вопросу: чем конкретно отличаются списки (list) от кортежей (tuple) в Python? Например, в изменяемости, синтаксисе или использовании.\"\\n\\nDIFFICULTY: medium  \\nTOPICS: Python, структуры данных', '[Evaluator]: ОЦЕНКА: 0.0\\nКОРРЕКТНОСТЬ: incorrect\\nКОММЕНТАРИЙ: Кандидат не ответил на заданный вопрос, а перечислил другие знакомые ему концепции, что демонстрирует непонимание темы или неумение следовать инструкциям.\\nПРАВИЛЬНЫЙ_ОТВЕТ: Основные отличия: списки (list) — изменяемые (mutable), используют квадратные ...']\n",
            "\n",
            "Кандидат: GIL — это глобальная блокировка в CPython, один поток выполняет байткод в момент времени.\n",
            "Интервьюер: Вы упомянули GIL, но это относится к многопоточности, а не к сравнению списков и кортежей. Давайте вернёмся к вопросу: в чём ключевые различия между списком и кортежем в Python? Например, можно ли их изменять после создания и как они записываются в коде?\n",
            "Мысли: ['[Observer]: THOUGHT: Кандидат не ответил на вопрос про отличия list и tuple, а перешёл к теме GIL, что не соответствует заданному вопросу. Это может указывать на непонимание темы или попытку уйти от ответа. Нужно вежливо вернуть к исходному вопросу, но, учитывая, что это уже второй раз, возможно, стоит переформулировать его или перейти к другой теме, чтобы не зацикливаться. Опыт кандидата (джун, 3 месяца) известен из первого сообщения, поэтому продолжаем технические вопросы. Сложность стоит оставить medium, так как кандидат ещё не показал знаний по базовым структурам данных.\\n\\nINSTRUCTION: \"Вы упомянули GIL, но давайте сначала завершим вопрос о списках и кортежах. Если коротко: списки изменяемы (mutable), а кортежи — нет (immutable), и это влияет на их использование, например, для константных данных. Теперь, чтобы двигаться дальше, назовите, пожалуйста, три основные структуры данных в Python и примеры их применения.\"\\n\\nDIFFICULTY: medium  \\nTOPICS: структуры данных', '[Evaluator]: ОЦЕНКА: 0.0  \\nКОРРЕКТНОСТЬ: incorrect  \\nКОММЕНТАРИЙ: Кандидат не ответил на заданный вопрос, а привёл информацию о GIL, которая не относится к сравнению списков и кортежей.  \\nПРАВИЛЬНЫЙ_ОТВЕТ: Основные отличия: списки изменяемы (mutable), кортежи неизменяемы (immutable); синтаксис: списки — квадратн...']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "state = start_interview()\n",
        "print(\"Приветствие:\", get_last_agent_message(state), \"\\n\")\n",
        "\n",
        "test_replies = [\n",
        "    \"Я джун, пишу код 3 месяца на Python.\",\n",
        "    \"Знаю списки, словари, функции. Декораторы использовал в FastAPI.\",\n",
        "    \"GIL — это глобальная блокировка в CPython, один поток выполняет байткод в момент времени.\",\n",
        "]\n",
        "for reply in test_replies:\n",
        "    print(\"Кандидат:\", reply)\n",
        "    state = step_interview(state, reply)\n",
        "    if state.get(\"final_feedback\"):\n",
        "        break\n",
        "    print(\"Интервьюер:\", get_last_agent_message(state))\n",
        "    print(\"Мысли:\", get_last_internal_thoughts(state))\n",
        "    print()\n",
        "\n",
        "if state.get(\"final_feedback\"):\n",
        "    print(\"=== Финальный фидбэк ===\")\n",
        "    print(state[\"final_feedback\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Сохранение лога в interview_log.json\n",
        "\n",
        "<small>save_interview_log записывает participant_name, turns и final_feedback в JSON; test_logger прогоняет сценарий и сохраняет в interview_log_{N}.json.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_interview_log(state: InterviewState, filepath: str | None = None, strict_format: bool = False) -> str:\n",
        "    if filepath is None or filepath == \"\":\n",
        "        filepath = \"interview_log.json\"\n",
        "    turns_raw = state.get(\"turns\") or []\n",
        "    if strict_format:\n",
        "        turns = [\n",
        "            {\"turn_id\": t.get(\"turn_id\"), \"agent_visible_message\": t.get(\"agent_visible_message\", \"\"), \"user_message\": t.get(\"user_message\", \"\"), \"internal_thoughts\": t.get(\"internal_thoughts\", \"\")}\n",
        "            for t in turns_raw\n",
        "        ]\n",
        "    else:\n",
        "        turns = turns_raw\n",
        "    log = {\n",
        "        \"participant_name\": state.get(\"participant_name\", TESTER_NAME),\n",
        "        \"turns\": turns,\n",
        "        \"final_feedback\": state.get(\"final_feedback\") or \"\",\n",
        "    }\n",
        "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(log, f, ensure_ascii=False, indent=2)\n",
        "    return filepath\n",
        "\n",
        "\n",
        "# saved_path = save_interview_log(state)\n",
        "# print(f\"Сохранено: {saved_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Запуск:\n",
        "\n",
        "<small>интерактивный запуск run_interview_loop и сохранение лога в interview_log.json после завершения интервью.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Приветствие ---\n",
            "Здравствуйте! Представьтесь, пожалуйста: как вас зовут, на какую позицию и грейд претендуете, кратко об опыте. Тогда начнём интервью.\n",
            "\n",
            "--- Интервьюер ---\n",
            "Спасибо, Алексей. Давайте перейдём к технической части. Расскажите, как бы вы подошли к задаче детекции объектов на видео в реальном времени, если есть требования и к точности, и к скорости обработки?\n",
            "--- Внутренние мысли (логи) ---\n",
            "[Observer]: THOUGHT: Кандидат представился кратко, но достаточно: указал имя, позицию и грейд. Опыт не описан, но для начала интервью этого достаточно — можно сразу переходить к техническим вопросам, как и требуется. Начинаем с темы компьютерного зрения, сложность оставляем medium, так как это первы...\n",
            "[Evaluator]: ОЦЕНКА: 0.2\n",
            "КОРРЕКТНОСТЬ: partial\n",
            "КОММЕНТАРИЙ: Кандидат назвал имя и общую область позиции, но не указал грейд (Senior упомянут, но не в структурированном виде), компанию (если она была в описании вакансии) и не дал краткого описания опыта, как требовалось в вопросе.\n",
            "ПРАВИЛЬНЫЙ_ОТВЕТ: П...\n",
            "\n",
            "\n",
            "=== Финальный фидбэк ===\n",
            "## Контекст кандидата\n",
            "- **Кандидат:** —\n",
            "- **Позиция:** алексей, претендую на Senior позицию инженера компьютерного зрения; **грейд:** senior\n",
            "- **Опыт:** алексей, претендую на Senior позицию инженера компьютерного зрения\n",
            "\n",
            "## Описание ответов кандидата по ходам интервью\n",
            "### Ход 1\n",
            "- **Вопрос:** Здравствуйте! Представьтесь, пожалуйста: как вас зовут, на какую позицию и грейд претендуете, кратко об опыте. Тогда начнём интервью.\n",
            "- **Ответ кандидата:** алексей, претендую на Senior позицию инженера компьютерного зрения\n",
            "- **Оценка ответа (0–1):** 0.20\n",
            "\n",
            "### Ход 2\n",
            "- **Вопрос:** Интервью завершено по запросу. Формирую отчёт.\n",
            "- **Ответ кандидата:** стоп интервью\n",
            "- **Оценка ответа (0–1):** 0.00\n",
            "\n",
            "## Краткие выводы по ответам\n",
            "- Всего ходов с оценкой: 1; средняя оценка: 0.20.\n",
            "- Ответов с оценкой ≥ 0.7: 0; с оценкой < 0.5: 1.\n",
            "- Затронутые темы: нейронные сети, свёрточные слои, компьютерное зрение.\n",
            "\n",
            "## Вердикт (Decision)\n",
            "- **Grade:** по данным диалога — см. описание ответов выше\n",
            "- **Hiring Recommendation:** требуется ручная оценка по описанию ответов кандидата\n",
            "- **Confidence Score:** 0 (отчёт сформирован по данным без LLM)\n",
            "\n",
            "## Roadmap\n",
            "- Рекомендуется оценить пробелы по описанию ответов выше и сформировать план развития.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'interview_log.json'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state = run_interview_loop(max_turns=15)\n",
        "save_interview_log(state)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## test_logger() — для финального тестирования\n",
        "\n",
        "<small>прогон сценария по списку ответов пользователя и сохранение лога в interview_log_{номер_сценария}.json.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Сохранено: interview_log_1.json\n"
          ]
        }
      ],
      "source": [
        "def test_logger(scenario_number: int, user_replies: list[str]) -> str:\n",
        "    state = start_interview()\n",
        "    for reply in user_replies:\n",
        "        state = step_interview(state, reply)\n",
        "        if state.get(\"final_feedback\"):\n",
        "            break\n",
        "    filepath = f\"interview_log_{scenario_number}.json\"\n",
        "    save_interview_log(state, filepath, strict_format=True)\n",
        "    return filepath\n",
        "\n",
        "filepath = test_logger(\n",
        "    scenario_number=1,\n",
        "    user_replies=[\n",
        "        \"Меня зовут Алексей. Претендую на Python-разработчик, Middle. Около 3 лет опыта: Django, FastAPI, PostgreSQL.\",\n",
        "        \"List — изменяемый, tuple — неизменяемый. List для динамических коллекций, tuple для ключей и констант.\",\n",
        "        \"Стоп интервью. Давай фидбэк.\",\n",
        "    ],\n",
        ")\n",
        "print(f\"Сохранено: {filepath}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
